# CTF Radio - Team Atlanta AI Cyber Challenge Interview

## Introduction

**Adam:** Hello everyone. Welcome to CTF radio. In another crazy bit of CTF radio trivia, we've now shattered our record for most number of interviews done and episodes recorded in a two week span. We're at five right now. So Yan, can you give the people what they want? We need you to bring that energy because we're doing all this recording.

**Yan:** Hello hackers. Why did you clap? Why would you clap? That's terrible. There's no need to clap. The clapping is great. The clapping makes it so people can seek into my part, which is of course where everyone starts. No, I'm super excited. We are basically recording non-stop. I see Adam on the other side of my screen basically every day, half the day or more. It's a dream come true.

**Adam:** But that should be normal. We work together. We literally have research meetings together.

**Yan:** That's true. But sometimes, you know, you can lock yourself in an office. And because you also have access to my office, I can also hold the handle. I have a key code. My key card opens your office. So I don't know. That was your choice by the way. I hold the candle and the handle and barricade the door.

**Adam:** Yeah, sure. You can when you're on the screen. There's no escape. I can call you. I can FaceTime you, man.

**Yan:** That's true. See, this is the beauty of our working relationship.

**Adam:** I was going to say our toxic codependence, but go ahead.

**Yan:** Exactly. No, it has been super awesome. We've been, like you said, spending weeks talking to all of the teams in the ICC and this session, it's the second to last one that we're talking to and then the last...

**Adam:** We have three more. We have one scheduled.

**Yan:** That's right. That's right. We still have one to schedule. We have one more scheduled and this is our...

**Adam:** Yes. Yes. Yes. But anyways, that's why I'm drinking coffee in the evening.

## Background and Team Context

**Yan:** But it's very cool. We mentioned this a lot. But in the Cyber Grand Challenge, which of course was 10 years ago, precursor to the ICC, the stories, a lot of the stories didn't get told. And there was an enormous amount of stories to tell. But of course after the competition everyone is looking to what the new thing is. Probably after the ICC have the teams are going to basically create the next massive AI startup or something and then you'll have forgotten about the competition itself.

But so it's really cool that there's this gap between pencils down - you're done writing the cyber reasoning system - to who won, and in this gap stories can occur. So today very excited to hear the story of team Atlanta who you see on the screen alongside us. Team Atlanta, the people involved are people that we have been academically friendly and collaborating with and hanging out with and partying at conferences for decades, definitely cumulatively and especially with some of the more senior leadership. So I'm super excited to talk to them.

**Adam:** Yeah that's great. Thanks for that introduction. So we have before we get into the team itself we have four great members of team Atlanta here. So why don't we go around. Taesoo, why don't you introduce yourself first to the good people.

## Team Introductions

**Taesoo:** Okay. First of all, thank you for your dedication. That's your amazing efforts. Okay. I really appreciate what you're doing. I always admire what you're doing for community particular AIC. That's great. I'm Taesoo Kim. I'm currently professor at Georgia Tech as well as VP at Samsung Research. Samsung research always want me to say those when I be present in public space.

At the same time team Atlanta as you can see from the name most of the team member are from Georgia Tech. There are many people, my PhD students, some people are graduated from Georgia Tech. So we are everyone's rooted from Georgia Tech in some way or another. At that many of the team member are leader in their own organizations. We are multi-organizational group consisting of KAIST, one of the university in South Korea and POSTECH, previously NYU and we also have some company involved like Samsung research and Samsung research America so we have an interesting mix of all these practitioners and in some of the programmers, engineers and many of PhD student and postdoc working together.

**Adam:** That's great. Yeah, I think most of the teams that we've seen are either kind of solidly either academic or industry based. So I'll be interested as we talk more of diving into like how that dynamic works. So great. Why don't who wants to go next to introduce themselves?

**Hanqing:** I can go next please. I'm Hanqing. I'm from Georgia Tech SSLab. I actually, I'm a CTF player as well. I played CTF. I started playing CTF from 12 years ago and I have ever played CTF with team blue Lotus, loop and later team deliverers. Right now in this AIC competition I lead a team focusing on C and C++ bug finding system. We have around five to six people and we made a bug finding system specifically for C and C++.

**Adam:** Nice. Awesome. What's your... Do you have a hacker handle since you're a CTF player?

**Hanqing:** Yes. My handle is hanqing95.

**Adam:** Nice. 95 because you're such a fan of Windows 95.

**Hanqing:** Oh, that's my birth birthday.

**Yan:** God, Yan, why'd you have to do that? You share a birthday, a birth year with Windows.

**Adam:** Well, not technically, but anyways, Windows 95 was the pinnacle of computing. Let's move on from Windows 95. I don't know why we're there.

**Yan:** Because of you.

**Adam:** All right. Great. Well, thank you. Welcome. Welcome. Minwoo, what about you? You want to introduce yourself?

**Minwoo:** Hi, I am Minwoo, a PhD student in KAIST advised by Insu Yun who's our team lead of the patching team, and I'm also part of the patching team. And there's around 10 of us in the patching team and I'm glad to be here to introduce what we did for around two years I guess.

**Adam:** Yeah. Awesome. Great. So if the patching works well, it was your success and if it went wrong, it was those other nine peoples, right?

**Minwoo:** It's Insu's fault.

**Adam:** Just not here. He wasn't able to join us today. All right. HyungSeok, you want to introduce yourself?

**HyungSeok:** Yeah. Hi, I'm HyungSeok. I'm currently working at a security research team in Samsung research America and previously I was a postdoc in Taesoo's lab in Georgia Tech. That's why I joined the team. At the automatic bug finding team especially for C, C++ and Java and while doing that I also put my many effort to integrate all the system and make them working.

**Adam:** So yeah it was not just working right, working together. That's like the hardest part is what we've been we know from doing it but also from talking to the team we can talk about lot of drama at the end also was we have to wake up every single member in passing teams like 5AM South Korea kind of the last minute hiccup.

## Team Structure and Communication

**Adam:** Okay, this is great. Okay, so then give us the... So we got a little bit of details. So roughly like how big is the team? I think it helps kind of frame things. So you have several different organizations and as you mentioned geographically distributed as well. So what's roughly like the size of the team?

**Taesoo:** We have about 30 people or 30 and more people working together across countries.

**Adam:** Nice. And is that are these people like all... I know some teams did like they had like a core of 10 and then they'd have some kind of people that would work part-time. Is this like 30 core people working full-time on this?

**Taesoo:** So we intentionally didn't divide the team members in that way. So we although half of team members are student, okay student is far from full-time, when you're talking to students. Yeah, I have a final exam they have to work on. Final project or research paper deadline. So extremely hard to deal with this part-time student. But I think they're smart. They can find their own needs in the team. They can play their own role. But I would say half of team member are student and postdoc, about 10 people or slightly more than 10 people are full-time engineer. I think this is one of the big strength of our team. I think I had a hard time convincing Samsung to dedicate resource so that we can at least reserve that much of time, that much of engineer for the duration of competition.

**Adam:** Yeah, that's really cool. And it also the PhD students kind of reminded me of what Yan talks about with the cyber grand challenge. I think that was kind of your approach, right? It was you were doing cyber grand challenge but also you were doing normal PhD stuff.

**Yan:** Yeah, it was just normal. You have a deadline like the cyber grand challenge quals is happening and there's a USENIX deadline. It's like exactly.

**Adam:** I think this time we are... I think I have to clarify a little bit. We are open track meaning that we didn't get the funding directly from DARPA to prepare this competition. In other word we have to find some of the resources to fund our PhD student which was tricky. We have to deal with this sponsor project while working on this competition as well. There's so much practical challenges behind.

**Adam:** Yeah, that's great. So can you talk a little bit? I think you mentioned it but was it and this is also why in case people were... Yan made the joke about drinking coffee late at night. We are recording this at 7:00 and the only reason I mentioned this is because you know I think to do this to accommodate your team right because you are geographically located. So how did you kind of structure the communication of the team to handle like were you all doing I don't know zoom meetings or was it hard to meet synchronously was it asynchronous maybe you could talk about the communication style of the team.

**Taesoo:** I think we also realized that communicating asynchronously through Slack is most effective way to communicate. We realized that working collecting all team member together. In fact this week is a workshop for the entire team member discussing and pitching about their new idea and what we did during the competition. Right now this internal workshop that we are doing every day until next week but collecting all the team member in the single place in a single time zone or near impossible. So that we intentionally divide team in a way that they have a similar time zone so that they can make smaller team get together and talk on Slack. The telecommunication system in Slack, they can just talk casually, but we try to reduce the amount of team wise communication as much as possible.

**Adam:** Nice. Yeah, that totally makes sense. And actually, I think we probably didn't get into it in the Shellfish interview, Yan, but I know that the Shellfish team kind of did that in essence because you had the three universities and so you'd almost naturally kind of coalesce one university like a subgroup maybe in there was working on the same thing because it's so easy when you're all there working on the same thing.

**Yan:** I think as the competition went on, it was more and more cross university just because they were more used to working with each other. But yeah, it's easier to work with people that are near you. My understanding is the early version of our patching system on shellfish for example is very much single university, two different single university systems and then they merged and grew together. The difference is though everyone's in the US on Shellfish and so Shellfish was able to gather very regularly in single some third party places right like big Airbnbs and stuff. I think you guys had an extra challenge there.

**Adam:** That's quite... we have a former student who's a professor in Korea and finding a time to meet that's not in the middle of the night for him and not in the middle of like right during dinner time or something for us is tough.

## Cyber Reasoning System Architecture

**Adam:** Wow. Interesting. Now let's get into the cyber reasoning system. Of course, the reason why we're all here. So, first question, what's the name of your CRS?

**Taesoo:** I think HyungSeok came up with the idea. Our CRS name is Atlantis the hero.

**Adam:** Oh, that's great. He probably asked ChatGPT.

**Taesoo:** I think that's the best we can think of. We are not very creative. So we leverage the creativity of LLM. One thing we actually realized during the competitions as well the creativity the human has is not comparable to the creativity the computer has. So I think creativity is not the unique thing for human.

**Adam:** Interesting. Wow. Bold words. Bold words. Okay. So Atlantis, is that a... is that just the name or is it like an acronym that backfills into something?

**Taesoo:** ChatGPT can make a backfill work pretty well.

**Yan:** Yeah, the sorry the question was if Atlantis is like an acronym or is it just the system is just called Atlantis and that's your final answer.

**Taesoo:** I think we vaguely call the entire system is Atlantis but internally there are I would say different maybe five different major component working together. I think I can briefly explain the architectures of what we did.

**Adam:** Yeah, that'd be great like as a high level overview and then we can dig in.

**Taesoo:** I think I can explain some historic aspect of our system as well. Previously in the semifinal major focus is a Linux kernel and user space program. So our CRS naturally divided into kernel focus which HyungSeok was leading and user space CRS that Hanqing was leading at the time and there was Java side as well. But when the competition evolved, kernel spaces naturally disappear because the compatibility is no longer OS project. So we retarget the entire kernel people into what we call multi-language systems that target both C and Java at the same time while keeping two specialized CRS for C and Java alongside there is a patching system universally handle both the requests in two different languages and there are sheriff team that handle the validity of the sheriff so basically five big team collaborating together in CRS development.

**Adam:** Nice. Okay so then what what were the kind of techniques that you employed like in those kind of at a high level...

**Yan:** Could we even back up a little bit? You said basically name three different cyber reasoning systems. Were they in the end three separate systems or a single meta system that had three massive components?

**Taesoo:** That's a great questions. We intentionally... I'll say our team relatively bigger than the other I guess so that we intentionally divide the systems in order to manner. If C space CRS design for C doesn't work we hope that multi-language CRS can handle. So these are three system working really independently. So there's a C component and Java component and there is baseline component where we call multi-language. They really working independently but there of course there is a API layer that communicate with the organization but internally there is a really separate component.

**Yan:** Did you ever do contests between the different CRS's because you basically did this three times. Hey you fired one of the teams that didn't do well.

**Taesoo:** Right. Exactly. There's only space for two cyber reasoning systems in our cyber reasoning system. I think that so one of philosophy that we have we have a benchmark systems probably any other people have as well. We can compare the performance between two CRS. But when it come to the two benchmark which are meet together, we intentionally make some decision to separate them out. The best outcome is that minimize the amount of common bug find like bug that we find. We like to minimize that but when everything is unified we hope that we can cover many more. So when it come to I would say specialized C work we push some of the techniques specialized for the C as much as possible for the multi-language team for example we try to incorporate completely different techniques when it come to bug finding for example like one great example is symbolic execution we integrate symbolic execution is one of the ensemble fuzzer inside the multi-language but not in the user space for example so I think we have intentional decision that multiple CRS cooperatively make the best result not overlapping or waste of resources at the end.

## System Integration and Architecture

**Yan:** Now what about and sorry to keep running with this this fascinates me the plumbing there's all this plumbing code maybe a different thing from an architectural standpoint. So you have these different systems, right? Can you... So there must be something at the top, right? That's controlling what gets submitted, which we can maybe get into. But then how much intercomponent communication like if the C component finds a crash does it communicate that to the multi-language component that it can then use that to find more bugs.

**Taesoo:** I think I can explain all this architectural relationship. I think it's good time for HyungSeok to jump on and can describe about our sheriff sharings and submission and patch.

**Adam:** Yeah, you had to get all this working together. That makes sense.

**HyungSeok:** Yeah. Let me briefly explain how our system work. So if the challenge program is given to our system then our challenge program manager component will be came up and then challenge program manager will launch C bug finding for C and bug finding for multi-language simultaneously and then if they find some seed then they're going to share the seed but if they find some POV then they will just submit to the challenge program manager then challenge program manager feature will verify the given POV really triggered the bug because sometimes our bug finding module sometimes missubmit the wrong POV. So to filter out such cases we intentionally re-evaluate the POV really triggered the bug and after that based on the given based on the some call stack we deduplicate the POV and if it is unique POV then we are going to pass that POV to patching module and sheriff module as well and wait for the generated patch and sheriff assessment as well and then submit them with some bundle.

**Yan:** Yeah. Wow, that's very cool. Jan, does that answer your question or...

**Yan:** Yeah, it does. I think the thing that so a lot of the teams we talked about I want to say all but I honestly don't remember. I think all wrote their own like queuing and plumbing system for communication between components. So you have these three CRS's do they all use different instances of the same plumbing system or did everyone make different decisions and you have three different plumbing systems to also integrate?

**HyungSeok:** I think we are sharing the same API for submitting and sharing the corpus.

**Yan:** Yeah. Okay, we are yeah centralized.

**Adam:** Nice. Nice. Cool. It makes sense.

**Taesoo:** I think some of the interesting decision that each of the team make one of those interesting part is that multi-language team make a deliberate decision not to do any compile time instrumentation. They try to extend the lib fuzzer and add other related code but not involve in compilation instrumentation on the other hand C CRS exhaustively do the instrumentations which mean high chance of failure for certain larger scale application so we have such a diversity in terms of design space that we made.

**Adam:** That's really interesting. That's fascinating.

## C/C++ CRS Deep Dive

**Adam:** Okay so then maybe we can start opening up these mini CRS's. So maybe Hanqing can you take us through like the C C++ CRS like what are the kind of steps... CRS is cyber reasoning subsystem? Wow.

**Hanqing:** Yes. So our C and C++ CRS is actually a pretty large scale microservice-based distributed system. You know in this competition we have a lot of cores, a lot of resources on Azure. We cannot do things like we did in the academic prototypes. Usually they are just a pretty small tool. So in this competition I designed an architecture to make it scalable to a large scale distributed system. You know there are some kind of challenges like how can we dispatch the fuzzers, the LLM modules and all of the utilities other components to different machines, different nodes, different containers. Also there are some challenges like you know the fuzzers are pretty fast but the language models are pretty slow. How can adjusted the performance difference. So finally our CRS is based on the Kafka based message queuing system and we all of the sub components communicates asynchronously. In that case we can easily support components having different languages. For example, our fuzzer is rust. Fuzzer is C, C++. But most of our controller modules are Python based in this case.

**Adam:** That was going to be my question. I thought everything was going to be rust.

**Hanqing:** This is what most of them are rust. But like that's another interesting things. We scan through the existing libraries that handle those hiccups in LLM systems like LLM libraries and what not. Many of them are just Python. There's no rust support. So we are constrained by the timeline and then we cannot teach everyone to speak Rust. So we have many decision that just adopt Python which everyone's familiar with. It's a first class support from the provider. So we had to make some adult decision given the timeline.

**Adam:** Yeah. No, that makes sense.

## Patching System Architecture

**Adam:** Okay. Then where does patching sit in all this? Is each is patching done in each CRS like subcomponent? Like do they each have different patching strategies or is there like a central patching system?

**Minwoo:** So like CRS patch has a separate name called Crete and the reason why we named it is we thought we were separate island and all those like requests for asking a patch for a given POV they come from all the other CRS C, multi-language or Java. So that's why we named it like that. So we have a separate CRS that does the job of making patches. So we get a detection like we name the detection as if contains the blob that actually triggers the vulnerability and our job is to make a patch for it. We have a big... our system has like about six to seven agents that each of them, each of them's job is to make a patch based on different strategies. And first we do the deduplication to check if the request isn't like a job already done and then make the patch and do a deduplication again to check double check if it's really not a patch already made and then yeah we submit it to the organizers and their scores I guess.

**Adam:** Nice. Okay. So wait this is a great interesting bit. So all of the patch like so you... is it true you only generate a patch if you have essentially like a POV first?

**Minwoo:** Yeah. Yeah.

**Adam:** That's an interesting decision we make. I'm very curious about how other people did.

**Yan:** Yeah. We know of one other team, no two other teams actually that were their systems were able... they one of them had a very LLM heavy system. And so they were able to generate they were able to generate like highly what they thought of as highly likely vulnerabilities and patches for something that they didn't know how to trigger statically. So highly likely vulnerabilities without a triggering input.

**Taesoo:** Yeah, we discussed about this like long time whether we... discussion or game theory discussion. Exactly. Technically feasible. If I'll say if your team's goal is place the top or bottom, you probably make a decision that just go for... yeah in the if diff is small enough like we can identify bugs easy right bug and fix you gamble. But if yeah basically gamble is really game based decision but technically though we evaluate internally that how much percentage we can actually make it correct in terms of patching. We realize after running all these too noisy. So we worry more about system-wise deduction score. Whenever you made a mistakes your score got deducted. This factor is so much if you think about how organizer designed this entire competition they're obsessed about practical whatever name of practicality right the real world practicality under the name they want some tolerance system and want to reduce the false positives as much as possible they're obsessed about it so that we also made such a decision. We can... if you want patchy system is actually made such a component enable. We didn't... they already prepared those component that handle without working POV but we disable it so that at the final competition that's one of the last minute decision we make here. Let's just disable this for the sake of competition but our system is designed to handle such cases.

**Adam:** Wow that's awesome.

## Patching Strategies and LLM Usage

**Adam:** Okay so then Minwoo maybe slightly more details about the patching strategy so were... Were you using just one LLM? Did you have multiple LLMs? What happens if or I guess I did you use LLMs? I must you must, right?

**Minwoo:** Yeah, we did use LLMs and at the start of this competition, we thought like we should break down the process of making patch into mainly two steps. Like the first would be creating finding where we should be fixing the bug. We call it the fault localizing and then once we get where to fix it, we then give the job to a coder a patch generator which should actually just fix the code which we think is the place that should be fixed to make the bug not trigger anymore. So we had different strategies of making of getting finding the localizations and a lot of strategies of how to write the code. But then the reason mainly I think the reason why we broke down this was at the earlier stage of the competition the LLM wasn't that smart. So we thought that we'd have to break down the progress so it could handle easy things and stack them up to a goal called patch. But time passed and the models got a little more smarter and even when we just gave the like the crash report and where you think we should patch it and how do you think we should patch it? It just does the job all at once. So we have strategies, agents that actually divide the process into both localizing and patch generation and there are also agents that just continue the context and the thinking into just a one step. I think this is a place to fix and I'll just give the patch right away and I think it should be this like this and yeah so we have around half and half I think that actually divides the process and doesn't and addition to that there are agents that try to extract properties from the codebase and tries to make the patch generated satisfy as much features that the code should have and things like that differ from different by agents I think.

**Adam:** Yeah. Nice. What so what LLMs were you using for patching and did it differ based on these two different strategies?

**Minwoo:** We mainly use Claude 3.5 and GPT-4 mini and GPT-4o. I think we thought that based on the vendors the models they would have a different strength like this would be good at this kind of strategies and this would be better at this kind but actually based on our evaluation we think that a better LLM model does just a better job in most of the things but we still had to like use a diversity of the models. So we just use different agents and LLM models in like a matrix and just trying to find a set that could cover most of the combinations I think.

**Adam:** Yeah. When you say you had to use a diversity, is this because of the spending limits, rate limiting or intellectual curiosity?

**Minwoo:** I think we could say that plus we don't know if just because that a model does performs better than another one it doesn't mean that the better one always solves all the problems. So we still have to give a chance to the other models if this could solve it. So yeah, similar philosophy to the CRS's.

**Adam:** CRS's is designed, right? Is like by you want to use both because sure there maybe there's some overlap but the unique parts of each is what you're trying to get.

**Minwoo:** Exactly. That's how each CRS will be differentiated at the end.

**Taesoo:** By the bar for finding vulnerability, bar for fixing vulnerability say 70% 80% everyone can get those number in day one but by adopting LLM next day you can get the patching agent but what about can you handle the last minute... the couple more this is extremely hard so in other word in our evaluation that after evaluating all this model one dominate... one more particular model dominate the entire stuff so that why not just putting all these resources for the one or couple very small set of LLM including Claude or GPT for example is our deliberate decision we made.

**Adam:** Nice.

## Custom Models and Fine-tuning

**Adam:** All right and one more thing about the patching module yeah when I integrate the patching system into our entire CRS I saw some custom model for patching module right so could you also explain about that Taesoo or Minwoo.

**Taesoo:** Awesome. That was going to be a question I'd ask later if you did any custom models. No, no, no. But let's talk about it now. We're talking about the models. That's great.

**Minwoo:** Yeah. Should I go for it? Well, yeah, you can go if you want. Yeah. Okay. So, there is an agent. Yeah, I forgot to mention it previously that uses a custom model. It does learning based on code bases and bugs and it tries to understand what we should give as the context for the LLM to fix the bug if I understand it correctly because I wasn't directly in the development but as far as I understood it's like it trains the model so that it could provide the appropriate context to the patch generating process. So it extracts symbols and things like that and trying tries to rank which is needed to make the sound patch and yeah it works like that I guess.

**Taesoo:** So I think I can elaborate a little bit more. It's a Llama 7B based instruction fine tuning that we did. The intention was that like again as I mentioned before one major larger scale LLM dominate we are looking for either we can quickly solve some tedious challenges faster than the other because there's a time-based scoring stuff this kind of incentivize the time but otherwise we have to take long time to validate whatnot. Our custom model designed to identify this common easy target quickly faster than the other. Another one is that we specialize everything for C particularly in the what type of symbol they have to look up so that given a particular repository and sanitization report and these are the symbol that you have to look. So we have to do a lot of instruction based instruction tuning for the model. So another interesting part that we've been experimenting with is that given a completely unknown repository can we do online adaptations against them is a little bit controversial it require a lot of resources during the runtime there are new techniques like LoRA like last minute the last level optimizations in the model way you can do things in 30 minute and 1 hour period so that you can do some optimization for completely unknown repository so that we can perform little bit better than the other. We couldn't evaluate effectiveness very well first of all we don't have unknown repository that right the model doesn't see, unfortunately but we have such a component in it. One of our this so far is unique among all the things we talked to is the first ML technique that is in query and LLM but like again let me clarify this is our hidden component there's a four people dedicated for the custom model. Our original intention is to adopt some of the Samsung's custom code model during the competition but again this is open source requirement things make it extremely difficult for the company to contribute that way so we that's why we adopt this Llama based one. I say this is one of distinctive decision that we can make as a team compared to others.

**Adam:** Cool. Yeah, it's really cool. We cannot sure it work well but still.

**Yan:** Oh yeah. I mean it'll be interesting seeing that trade-off, right? I mean because yeah like you said it you know taking a while to so you're actively fine-tuning it on the repo that you have. That is really cool.

**Taesoo:** Yeah. Online modification.

**Adam:** Nice. Wow. Awesome. I'm very excited to see the results here. This is that could I thought we I thought I mean we had talked about and hypothesized that everyone would have roughly the same shape of CRS but that is definitely not the case. So there's a lot of and there's a lot of variance even within I think the CRS.

**Yan:** Yeah. And in fact, so far we've been hypothesizing the two shapes, right? Because we have the cyber grand challenge CRS with some AI splashed in, some LLM querying splashed in, and that's the people that live in the program analysis world and visit AI. And then we hypothesized an AI first approach, which is something closer to THEARYS, for example, right? That they're really an LLM focused crew. And then we thought, oh, that's it. Those are kind of the two guys. And you there's a there's a people like us like LLM wannabe.

**Adam:** Yeah. But you're actually AI researchers.

**Yan:** Yeah. So, Oh, that's super cool. Wow.

## Multi-language CRS Component

**Adam:** Okay. So, then maybe we can hit some of the other components. Like I think I'm particularly interested in like the multi-language like CRS component. Yeah. So so how does that work?

**HyungSeok:** Actually I will... Primary goal is to support multiple languages. So first we need some abstraction layer for executing the any language like JQF or libfuzzer based harness. So to do that we make some hole in the JQF and libfuzzer by using shared memory and some semaphores. So to do that we can easily get the some coverage feedback and easily execute the input what we want to actually execute. So after that we focus on how to leverage large language model to improve the fuzzing process. So we especially we focused on the how to generate the better input by using large language model. So to do that we did some LLM based dictionary generation. So for example if we give the a given if we give some function ask LLM to generate some interesting dictionary and based on that dictionary we can do a dictionary based input mutation in the function level. So previously we usually do just collect the all dictionary and mutate inputs based on the old dictionary rather than function level dictionary. And we also by using a large language model we also reverse engineer the input format and based on the reverse engineer the input format we do some grammar based fuzzing.

**Adam:** So nice. So before yeah before doing that we tried to utilize the previous some static analysis based reverser but yeah the quality of their output was not that good. So we switch it to large language model to reverse engineer the input format. And also we have some heavy modules called MLA. So basically that's we basically we first ask LLM to figure out which line will be the bug candidate and then we ask LLM to generate the Python script that can generate the blocks hitting that lines or triggering some bugs. But after doing that we realized that just bug generation script is not enough. So we ask LLM to generate the input generator written in Python and also input mutator written in Python as well.

**Adam:** So after and the mutator. Oh, that's yeah mutator as well. So this been plugged into the fuzzing loop essentially like right.

**HyungSeok:** Right. So generating the mutator is not something I've heard of before. That's really well. Okay. At least not with that in this in this way. That's really cool.

**HyungSeok:** Yeah. Yeah. So, this is something we're going to publish. Okay. Hands off. Hands off. Yeah. Basically, we ask them to generate some mutator that can mutate the input that already trigger the specific function to the target function. So, yeah, in that way we can improve our fuzzing process a lot. Yeah. And also we try to leverage directed fuzzing as well. As I mentioned large language model could point out which line is suspicious lines. So based on that output we can guide our fuzzer to reach that target lines as well.

## Directed Fuzzing Approaches

**Yan:** So that yeah how do you guide the fuzzer in your CRS? One thing that we ran, the reason I asked this question, we ran into this this very poor quality of directed fuzzing techniques and we had to have several different techniques on the in the artificial CRS to try to do directed fuzzing.

**HyungSeok:** Nice. In in CRS in multi-language component we are also not that good because if we do better we need some core graph a control flow graph to calculate some distance between the target line and current covers but I thought that we do not have enough time to do that so what we did is just calculate the score based on the coverage and the target length for example Example if the target if it so basically our bug candidate has not only target line but also should be taken line which is a kind of path for triggering that target line. So based on this information we can calculate the seed. So for example if the seed touch some should be taken line we give the seed a two point something like that. So based on that score we just do directed fuzzing. And also if we find some crash or POV then we deprioritize that bug candidate because we already found that bug so we do not have to focus on that line. So after that we deprioritize that bug candidate and not to re... yeah not to...

**Adam:** And this is deprioritizing is basically you know you reduce the frequency with which that seed is selected for mutation.

**Taesoo:** I think we have another directed fuzzing component in the user space. Hanqing do you want to explain a little bit more or...

**Hanqing:** We also utilize the directed buzzers. We actually have two cases can invoke the directed buzzer. The first one is actually the static analysis report assessment. When the static analysis team sent us a report and a target line a suspicious line, then we launch the we launch the directed buzzer to verify if the target can be can be triggered. Another case is the delta mode. For the delta mode, we firstly call large language models to reason and aim for some suspicious code locations and then we put it into the directed fuzzer or some kind of directed fuzz.

**Taesoo:** So I think we have a little bit serious directed fuzzing component in the user space that divide the entire core graph the control flow graph in the multiple step in a way that this is a notion of the graph centrality notions in all these node if you reach here they're going to expand the coverage significantly we select those component try to divert try to direct the fuzzer one step at a time but in theory it work great but in reality instrumentation fail all the time. That's a reality with the theme of competition. So, but we have those component deployed in semi final version of the CRS we submitted the performance is not guaranteed but we have those component and this was like the this is on even on the source code level like inserting those annotations is difficult right?

**Taesoo:** Yeah, exactly.

**Yan:** Just to clarify for the audience because they may think well yeah binary instrumentation is like hard but this is you know even and Jan I think you got ran into this as well.

**Yan:** Yeah we ran into this this exact thing like source annotations and then everything blows up.

**Taesoo:** So particularly the form of competition can be more regularized in a way that they can promote innovations in that level instead of dealing with this well again what the name of practical real world software right but we like to figure it out what's a impossible stuff possible with LLM but there's so many things that we have to deal with like you probably experienced a docker version changes like our system completely failure that not even working the baseline changes is out of sudden everything change everything break but this is not what we want to validate through the competition so it kind of limit our exploration capability during the competition.

**Adam:** Yeah and this I think is something that we've heard a lot in these sessions now as well I think on the other side of it one of the big criticisms of the cyber grand challenge was that it was done on toy operating system with toy programs.

**Taesoo:** Exactly.

**Adam:** Architectures, seven bit architectures and so you know figuring out that middle ground, but AIC went almost all the way in the other direction, right? It's like, you know, you have to...

**Taesoo:** Yeah. So it's super hard the nature.

**Adam:** I think the other thing that's come out of a lot of these interviews is each team kind of had to make that research versus engineering tradeoff, right, of like how much you know, Jan's big push on our team was to try to get them to do something simple that just kind of worked which they completely ignored and did something else. But like Jing Yu's team 42 Beyond Bug their whole they wanted to test test test like they deliberately didn't do crazy researchy stuff because they wanted to make sure that the stuff they were building they were confident would work. So yeah it's just a crazy you know it's a the reality of the competition the reality of real world code right all this stuff kind of and kind of comes together.

**Adam:** Wow cool. Okay so are there any components okay are there any components we haven't touched on?

## Java Component Challenges

**Adam:** Is the Java component CRS is it kind of similarish to the C? Like did you have a Java specific one? I can't remember. Was that the multi?

**Taesoo:** Yeah, there's a Java specific one as well. But I think in terms of design, not particularly different from what we are doing in the I would say C space. But there are several things that we like to clarify. Fuzzing in Java is a little bit tricky. We hate Java.

**Adam:** You're off of rust. Taesoo says no rust. Rust sucks. It's all Java from here on out, right?

**Taesoo:** Okay. Java is pretty unique in on the other hand like all the software engineering community work on Java for decades. Decades is not right word. It's like half century per se. We assume Java has symbolic executor. Symbolic executor is really based on Java. Everyone is work on Java in the software engineering community. In reality not working. We spend tons of analysis on top on top of Java. There's so many Java JVM dependencies and crazy library in dynamic nature of the Java like fuzzing near impossible to win in practical software manner. We have to deal with JQF right there. There's a saying that the fastest path to insanity is Java Pathfinder.

**Yan:** Yeah. Right. By the way, one thing that we observe there's a pretty interesting thing is there's no memory corruption. Memory corruption can happen anytime in the C program. On the other hand, in Java, there are certain places that fuzzer can trigger like there is sanitization hook and like hey there's a command line injection there are some like cross scripting things and like there there are certain places that or or reflection class loading deserialization stuff there are certain places that the bug what we call security bug can only happen. I think this make our design of the project very unique in a way that we can divide many of those component into tools like explorations of the code coverage and exploitations to reach the bug in a way that you can put some of for JQF for example you have to you have to put intentionally unlike in the C we try hard to trigger memory safety issues but in Java it's all about how to make JQF happy. We find the input JQF can realize this is a buggy. So all the reasoning behind this Java is how to make this JQF how to understand how JQF behave how to JQF recognize those and patching size is also tricky too that how to fix the bug in Java. Hey, create another exception routines like hey what about we wrap around the entire entry point of the Java hey catch the exception no like escapes by nature it's just secure languages it's all about how to adjust those I'll say practical stuff at the end but if you see the actual patch that they provide as a sample right if there's a out of array bound access create the exception routine that prevent this and return. It's pretty in some sense extremely easy to fix in that way but probably that's not what developer want but like there are several challenges practical challenges in that regard specific for the Java as well. I think the quality of the Java output is not desirable. I would say if I'm the Java developer probably you have to think completely different way in terms of writing patch and whatnot. But because we are CRS automatic agent can just deal with this particular instance of the bug. There are so many easy way in Java unfortunately.

**Adam:** Yeah, that's interesting. I mean it it kind of is surprising like you said. I think I'm kind of struck with the you know looking at the software engineering community. There's so much research on like Java and yeah everything is Java like creating code life in Java.

**Taesoo:** Yeah. We thought this is like hey there should be tool like people working on but none yeah and even though you know obviously C and C++ are older but I feel like those fuzzing techniques are much newer than Java right I mean the there's a difference of communities too I think security is so focused on like that the real world impact the final thing we've found these bugs and you know software engineering you I have increased code coverage but it doesn't quite survive reality. The state-of-the-art patching schemes from software engineering community is all about how to deal with null pointer exceptions in Java which security community don't care that much about right so I think the bar here is slightly different all expectations of the competition is slightly different than what software engineering community care about but it's all about security critical one and so then you you kind of...

**Adam:** Good. I was just going to say like Jan said the the security community and the even the hacker community I think has focused maybe over-focused on C and C++ targets and being able to fuzz those whereas kind of Java languished be in my mind mainly because like Java targets like J2EE these are like big enterprise apps that and there's not a lot of open source of these apps as well like a lot of them are closed source and that makes everything more difficult.

**Yan:** Yeah, I think it's pretty fascinating to think about, but yeah, Yan, you were saying from what to put research, what to put time on in the competition again, Java kind of or anything that's not C or C++ kind of puts us in this interesting situation of do you want to reimplement like the fuzzer you know or or because you could implement the AFL++ for Java, right? And and and and then take all that time or you could try to make JQF work. I think because the JQF was the reference, everyone basically went with it. I mean, you had to because then if you submitted something, they wouldn't crash.

**Taesoo:** Yeah, exactly. You'd have to translate it anyways. So, yeah. I don't know. It is a little bit frustrating in that regard. I don't know what other language would have been better though.

**Adam:** Yeah, exactly. The usage of the Java side particularly in the hospital involvement enterprise are humongous. I agree but at the same time fuzzing probably the not the best possible security tool in the Java probably they need to like the language is memory safe by default but there are some Java specific one that we're looking for probably fuzzer is not the best one for the job but at the same time there is interesting JNI component not sure you guys handle or not we is a multi-language multi-language aspect that we monitor address. There's a Java side, there is a C side that they're going to native code from the Java side. We are very ambitious to handle those. We ended up giving up.

**Adam:** Oh no. Yeah, I was going to say we did some research on that on the Android side of stuff and it it's very hairy.

**Yan:** Yeah I remember the the anger support for that even like for symbolically executing through Java and to C it's absolute hell all the subtleties.

**Taesoo:** Yeah, we have those support in mind because organizing explicit say there's a possibility that multi-language can can appear during the competition they clarify you have to modify the language the harness is specific the project specify hey this Java plus C which language are you're saying should we handle like entire because is a Java based program that invoke C written in JNI. Right. Then how to handle this C program that potentially buggy which is not part of the scope. So there's so many practical things that we have to care. So we ended up...

**Adam:** Yeah. Wow. Yeah. That's interesting. You have to you have to scope it for you. That's why I think I mean part of that that we've talked about before too on this podcast is like the competition itself is also trying to hit a moving target and you know is trying to stay on the on the cutting edge of that better suddenly.

**Taesoo:** Yeah. So it's just crazy. I think probably looking back if we have the organizers on on the podcast and we ask them what would you have done differently it's very much easier to see in retrospect right so I think you you whenever new sample challenge appear our mindset completely change so there's a back door in the program there's a timeout which we don't care as a security researcher. It seems like this is what they care. So we have to start support out of memory issues as about time like this is like like open our mind into what what have to work on. Actually this is how they drive us what what we have to focus on. We all about hey we have to deal with the use-after-free these most difficult challenges that everyone care about but now you have to handle this all these little one as well. So yeah.

**Adam:** Yeah, it's really interesting.

## Secret Sauce and Philosophy

**Adam:** Okay, cool. So let's so what would you say I think we've talked a lot about your CRS and I loved all the details. So what would you say and because we've touched on a lot and maybe we've already touched on it, but what's like the secret sauce of your CRS?

**Taesoo:** Actually, I'm just going to stop there. I think I think we can make turn and can talk about something. Yeah. Yeah. But I think the diversity of our CRS is I think our secret sauce. One component might fail as we saw in the in the multiple rounds. Other people CRS might fail. But we hopefully our CRS one component fail other people other CRS can take over from here. So that there should be no missing simple bug that we are missing during the competition.

**Adam:** That's great. Yeah. And is Yeah. And that's that's a almost like a yeah diversity of designs even of the systems, right? Because you have like redundancies built in. It's not just I know a lot of people think of diversity like yeah you just run a second fuzzer that's you know that's great but it's a same copy of the same fuzzer running on the same target. Like that's not giving you anything in terms of robustness to of your system.

**Taesoo:** So another secret sauce... a little bit controversial but when I when I pitch this idea to Samsung in a way that hey we we need the systems we're going to deploy this one after the competition when everything is open source hey give me this amount of engineering engineers or I'm going to hire everyone for this competition their KPI whatever whatever performance metric if you don't win we're going to get fired what was very strong argument if you if you don't qualify for like we're going to quit. So, they're really doing their best. This is one one of secret sauce we have.

**Adam:** Nice. Motivated people. I like it. That's great.

## Impact on AI Outlook

**Adam:** Wow. That's awesome. Cool. Okay. Yeah. So, then I think we've touched on it a little bit, right? Even you touched on like models and the the ecosystem changing for AI. So, how has for each of you how has your personal outlook on AI changed by participating in the AI cyber challenge? Were you skeptical at first and then were more maybe Taesoo you want to take that and then we can go around?

**Taesoo:** Yeah, I can talk about us. We are the skeptics particularly semi-final our our motto after evaluating existing LLM what not our motto at the time hey we're going to we're going to deal with this competition we're going to show the powerfulness of traditional technique so that hey we don't need and by then you had a customizable on the fly fine-tuning now AI wannabe okay but like at the time even when we preparing for the semi-final if you see the I think we're going to share this semi-final codebase as well but I believe if you see our code base we don't leverage any LLM no LLM allowed for the final semi-final except Pentesting of course so we are skeptic at the beginning but everything changed like hey there's a new code interpretation happened like now AI can do something crazy and reason everything about Python. Hey, now there's a new function call tool calls or not. So much easier to deal with this weakness of LLM by using current status code tool. So I think there lot of interesting component we couldn't do by using traditional techniques particular static analysis and stuff the LLM by default outperform the most of the static analysis that we have particularly given this complex software.

**Adam:** Yeah, I mean it's crazy. It's really impressive. And I think for for people that that don't know Taesoo has a very prolific career in in security and program analysis and and all kinds of areas. You know, when you say that it's actually better than those techniques, this is a real reasoned and like your background really speaks to that. So I want I want the people to know that that it's this isn't just like an uninformed opinion that like man vibes they're better like no no it's real right.

**Taesoo:** Yeah. So one of yeah one of the any total example right now in our lab is that when people doing reverse engineering or the time of hidden binary like unknown binary now people just copy and paste assembly to ChatGPT they realize it's better than IDA better than like JEB or what not they start relying on LLM every single workflow that we have in terms of reverse engineering or traditional software analysis it's a new...

**Adam:** Yeah. Yeah. It's really crazy and and it's I think that's the beauty of this competition especially because it is in many ways kind of the the next level of the cyber grand challenge, right? is you have if a bunch of AI bros came in and you know were saying yeah it's a new world it's just a bunch of people saying this but we have seven teams full of some of the top program analysis researchers in the last several decades and they're saying this and that's I think that's one of the biggest values of of of this AIC.

**Taesoo:** Yeah exactly. We are joking all the time. This best outcome of AIC is to convert the top hacker to the AI domain. Just expose that. Yeah. It's not about the AIC CRS. What now? They're going to be outdated pretty quickly. This pace of AI is crazy. Now we open up. So we start working on those crazy stuff at the time and start integrating those our job.

**Adam:** It also speaks to like the AIC as a competition catching wind right when these models because just like you said Taesoo I think us is very similar. I was incredibly skeptical when when this came out, but I was like yeah, we should definitely do it. That that sounds like fun. Like why not do it, right? And like sure if fuzzing and program analysis is the best approach. Like that's awesome. But but then you know but then even like towards quals the students showed us some results and you're like wow it seems like it's kind of working and then as things got more and more it just got better and better and really kind of took off to this level level where a new model would come out and then boom we're patching things that we weren't able to patch before and it's just wild to me.

**Yan:** Yeah. It's Go ahead. I I had a lot of conversations with kind of DARPA PMs and and so on in the in the interim between the the cyber challenge the ICC and I think one of the things that people point to is this exact effect back in the original cyber not non-cyber grand challenge the DARE held for self-driving cars right in the early days of of these grand challenges one of the biggest side effects that first challenge. No car made it to the finish line, right? It was like from some perspective it it was a failed challenge, but suddenly you had all of these top researchers that were now enthralled in self-driving cars and that leads directly to me taking a Waymo on the way here today, right? So, it's like, yeah, it's awesome.

## Engineering Challenges

**Adam:** Wow, that's cool. Okay, I actually have an engineering question before we get to the end. We got about a little bit left. So, you mentioned this big kind of a big system, right? You have several CRS's running. So, what was like the local dev story? Could you run each sub CRS as its own thing locally? Did you have to run the whole system? Like, how did how was that dev experience like?

**Taesoo:** I think HyungSeok is the best person.

**HyungSeok:** Yeah. Yeah. Personally, it was horrible to me because I was to test. Yeah. So basically I ask every single sub teams to run their CI and test a lot before integrating into a single unified system. But when I try to pick them and integrate into a single CRS and run it on the top of Azure it does not work. So I every day I wake I asked the developer of that CRS hey here is the bug so we should fix it something like that. So yeah it was very horrible.

**Taesoo:** I think one thing that I observe but I'm not using cloud all the time because I'm just academic I like server not not cloud I just I just give me the root bare metal access I going to do everything by myself don't touch it. Okay. But one thing I realized is that all the members complain. Yeah. It work great like perfectly in my local setting when it come to Azure everything break down. Yeah. Oh, we even test much larger scale in the local systems. But when everything migrated, everything break down like magically some name server like got dragged some of the uncertain file system slow down like and then network file system is suddenly very slow like there's so many uncertain stuff in the Azure space that we couldn't like make our CRS happy.

**Adam:** Oh, I think HyungSeok is the one got all the blame or the glory for bringing it all together, right? That's kind of the...

**HyungSeok:** Yeah. Well, I feel your pain. I was I've told this before, but for quals, that was my job is they the students messaged me like a month before the deadline and was like, "Hey, we don't have any way to like test our system. Like, we we should really be doing tests like on every, you know, commit." And I'm like, "Yeah, you should be. Why why aren't you doing this?" And so, I had to wait. It took me three days to figure out how to even build the and run the system locally on my machine. Like it was so complicated and there was so much institutional knowledge among the people that it was wild. And then taking that knowledge and coding it and the CI and getting builds running and of course those aren't big enough and you got to run your own clusters and it was just but that was for quals and I was like I'm done after this. This is I can't I can't do that again.

## Critical Bug Discovery

**Adam:** Wild. It's a good time to talk about one of one of the bugs that we experienced. I think probably HyungSeok can describe or Minwoo you guys know the context about the fuzz keyword that we rely on in the patching team.

**Minwoo:** I know that. So basically the rule said we should not fix the harness because if we fix the harness then everything will break. So we should not fix it. So to mitigate that in the patching patching module explicitly specify that if the file name contains the fuzz then it should do not modify that file but there was some case that the challenge program name has fuzz something something like that. So yeah, the last last minute integration test that we got start with OSS fuzz and name.

**Adam:** Oh no, not sure you guys like you know you know that the previously NGX. Yes. Yes. Yes. Yes. I you know we didn't run into a bug with this but I remember like scrolling through our like CI logs and it's OSS fuzz dash whatever and then there was whatevers it just I remember now it didn't like there's a OSS files and project name it's like our CRS patching CRS never produced anything in this oh no this is so critical that we have to wake up every single one of them at 5...

**HyungSeok:** Yeah, I'm fixing it. It was right right before at the time I already rev up everything and I already submit our final version. But I feel like something weird. So at the time I check again everything. Oh, stop.

**Adam:** So this is what like a day before submission or the night before?

**HyungSeok:** No, maybe six six hours before the final submission.

**Adam:** Wow. So I just... Hey, you guys submitted really early the first time.

**HyungSeok:** Yeah, right. Yeah, that's our goal. We last time everything is one day before. It actually almost work out. Almost out. Well, it actually it did. It saved you.

**Adam:** Yeah. Because if you had done it the last minute, there'd be no time to fix it.

**HyungSeok:** Wow. Okay. So, yeah. So, 6 hours beforehand is when you found that bug.

**Adam:** God, I'm getting flashbacks. I remember walls. It was like the day before, the night before the submission before they extended it and one of our I they'll remain nameless, but they committed some code that was looking they were testing some feature and they wanted to drop a file on their local file system to turn it on or off and they ended up committing that code of looking for a file with their full home path and it just like so I think that was testing if we would submit patches and so we never would have submitted patches or something in the in the quals and thank god like somebody found it at some I I can't I think somebody's going through the logs and being like, "Hey, this run of CI is not submitting anything." Why is that? Well, interesting. By the time that like the competition finished, we had a very lightweight analyzer that would use the various LLMs on every PR and and tell us like, "Hey, this might be an issue. This might be an issue." It was very useful, especially that final post.

**Yan:** Oh, that that's interesting. Well, it wasn't a great issue. Do we actually tell the whole story of what happened 45 minutes before the submission?

**Adam:** We can we we should probably leave that for like a retrospective episode. I want to make everyone feel comfortable that we also made these mistakes, but we... So, okay, just real quick story. We submit. We're done, right? I I you know, I happen to be milling around looking over people's shoulders because I can't not do that somehow. and our well actually we we can name and shame because there's no shame because it wasn't her fault but it it is handling the the final submission and she says okay and this log she's looking this looks good this looks good this looks good and then as she hits Q out like to quit the the whatever less right I see a exception pop up and I'm like oh wait wait wait go back and she looks at And it's in the like main the plumbing. It's in the plumbing. And she says, "I've never seen this exception before." And it's 45 minutes before. And meanwhile, there's like 20 people there. There's like three people actually like working on computers. Everyone else is just kind of waiting and for the final like we've submitted. And then we hear that I've never seen this error before. And it just gets deadly silent. And it was just it a last minute thing got submitted committed that raised I think we might have talked about it that raised the timeout to beyond the range that Python can handle because we're like we don't want the critical things to time out so let's let them run for three billion minutes and it just kept blowing up. It was a very stressful time manually edited for various technical reasons. We couldn't do a full system redeploy. It was like it was it was it was special.

**Taesoo:** Think about we developing complete autonomous CRS assistance. One line, one token just destroy everything. We couldn't deal with this.

**Adam:** Yeah. Yeah. the it's interesting because it's kind of a you know I was trying to think that that's like a very almost contrived scenario right there's not many scenarios where you just ship a system and you're completely hands off and it just works right you're a company you develop software develop a website you can push fixes all the time you can release new versions but you know it makes sense for the competition but man that like not knowing or like these close call situations that we're talking about that really gets you like right here and and also add makes everything nondeterministic. So in our case in our case I at the final after submitting our version I also tested but we failed to generate the patches but I just thought like it's because our nondeterministic LLM so I just went to bad to sleep but after waking up I just feel like something weird and then check again everything and then we realized that we have a bug issue.

**HyungSeok:** Oh wow wow wow wow. Yeah, that's yeah, this it's and they're very complicated systems too, just like you said, like well yeah, maybe okay, sure, normally we patch whatever we'll have two patches at this point in the testing, but like like LLMs can be weird and other scheduling could be weird, fuzzers are nondeterministic, right? All this stuff is nondeterministic. Our drives have finite storage and if you want to run this system for 10 days straight and it logs anything, you could get right? Like it's yeah it's terrifying and and here's the reality something is going to fail right like you just have to hope that it's in a non-critical component and in the cyber grand challenge every single CRS like this enough non-deterministic or redundancy into your system that like I was going to think of the Jurassic Park life will find a way but your CRS will find a way right there'll be some path to some bug that's great.

## Competition Predictions and Motivation

**Adam:** Okay so now got this like like close call like hearts stopper out of the way. So, what place do you think your CRS is going to get?

**Taesoo:** Sorry, did I cut out? Top three. Yeah. Otherwise otherwise people lose their jobs. No, I'm so sorry but this is our KPI. But yeah we we wish for the best but I I think the our our motivation at the even at the beginning is don't get be shamed by not being top three is not about money is not about everything in fact Georgia want us to donate the rest of the money so we are not getting penny out of it but we still win the competition otherwise not sure I going to survive in the security company so we We really did our best last one and a half year. I'm very proud of my team members and all these lead working like 24/7 all the time.

**Adam:** Yeah, it really becomes kind of like an all-consuming project, right? There's and like you said, there's so much stuff you could do, right? Especially as the models get better and you get more ideas, but you have to temper the the crazy like I want to develop this crazy new component versus I'm going to really make sure this thing works.

**Taesoo:** But I think one thing that worry me the most is that AIC for example like paper review research and I've been rejecting or decline all these requests say I going to focus on AIC this is the main things last two years I'm not doing anything else but now I have to find something to work on so you don't have to review papers that's what you're saying I have to study paper I I kept telling our students on the last last couple of days.

**Adam:** It's like after an event like this where you focus your entire life for this long, there's no alternative to just being lost and depressed for some time. Like in the cyber challenge happened to everyone, the the winners, the losers, anyone. And yeah, that that is, you know, by the end of the CGC, I was pulling these insane 32-hour days where I would work for 24 hours straight and then I'd go home and sleep for eight. And so I'd like shift over time. And that was like my entire everything was in in that competition. And then it was over and I wake up after Defcon and like now what? What am I gonna say? Nothing to do. You're a PhD student, man. Write papers, do research.

**Yan:** Yeah, it it takes a while to to remember how to do that.

**Adam:** Yeah. Oh man, that was awesome. That's so funny.

## Post-Competition Plans

**Adam:** Cool. Okay. So then what I think you've kind of talked about it, but maybe you could talk about what's kind of the plan for you know post is the cyber reasoning system is something going to happen with it are you guys running with it? Is Samsung going to use and commercialize some of it? I think we're all interested in like you know there's obviously the things will be open source but we're also interested in what the teams are kind of going to do after the competition.

**Taesoo:** I think that's a great question. The research side that we we have a list of researches subject we want want to explore on top of CRS they're going to continue but in terms of real world deployment we are actively working on deploying our CRS version for internal Samsung repository the same open source version we create some UI in a way the developer can use this one we extend a few things in a way that we can involve human in the like human in the loop process doesn't have to be completely autonomous. Human can give some hint about the patch they want to generate and etc. that we are making the systems not complete autonomous semi-autonomous way internally for internal repository that we have inside the Samsung. So that's great workshop right the workshop. Yeah basically workshop that we are doing so that they can share the internal structures as a former manner. But we already created those front end for the CRS so that our deadline for internal deployment is near October this year.

**Adam:** Wow, that's cool. And how are you handling that it will live on? It is living on.

**Taesoo:** Yeah, like they're already planning for it. That's great.

**Adam:** So, can you and of course we understand if there things are proprietary and you can't talk about it, but can you talk about how you're handling the harness creation problem? I think for me that's one of the hardest parts of applying fuzzing to any new target even when it's me doing it.

**Taesoo:** That's a great question. Great question for the community and OSS fuzz project has a some form of automatic harness generations and we this is one of our internal agenda for next year. So once we are done with this we're going to automate the process of harness generation but harness is a little bit tricky. But currently there's so many existing harness for the internal repository that we'd like to improve the quality of the bug finding and we can also provide the patching candidate so that automatic agent run overnight and provide the PR for the corresponding researchers and developer.

**Adam:** Wow great cool.

## Lessons Learned and Retrospective

**Adam:** All right so we're kind of getting close so looking back over the last year and a half two years what would you have done differently? So, is there an approach you'd take differently or I don't know, even a team structure? It's kind of an open-ended question of whatever you think, you'd do differently. Like looking back, do you want to mention we have like jillion list of what we would do differently?

**HyungSeok:** Yeah, says get somebody else to run and test this damn thing so I don't have to do it. Anyone? Yeah. Any anyone want to to describe what you wrote it down for our workshop? You can you can go ahead.

**HyungSeok:** I think the first one is building the high quality benchmark at the beginning rather than building them later because it drive us to develop our strategy and our system. So yeah but in our case we yeah you know the the format of AIC competition is keep changing. So we we should change our benchmark as well. So it was very... Yeah. Yeah. So yeah, that that's the first one. And also yeah, let me check. Oh, I think we better to explore more how to utilize a large language model better than now. For example, so in our module, we give coverage feedback to large language model to generate better generation script or mutation script. It is quite hard because we have some context length limit and we have have some TPM and we have some credit limit. So how to efficiently provide some these coverage information or other very useful feedback to your large language model to make them better such kind of things. Yeah, we have a bunch of things related to large language model things and nice.

**Adam:** You just have to convince Samsung to build their own large language model now. So, we have...

**Taesoo:** Yeah. All right. Nice. I love it.

**Adam:** Anybody else have any things that they they'd do differently or approach differently?

**Minwoo:** As for me, in the patching team as you rely a lot on the LLM. I think I if we knew that LLMs were going to change this rapidly during the competition. I think I tried to keep in mind that this is going to change a lot because during the competition we made things for the previous models because we thought we they're going to need it. We did our best at making them but after a few months a new model comes up and it can just do it by itself. So we just lost a lot of like development time and I think that was very sad I guess. So trying to keep in mind what could be changed and what is going to be definitely needed and trying to keep that in a list and do that in a order priority that would help a lot in the competition.

**Adam:** I guess it's a very interesting position to be in because you have to kind of gamble on technology progressing at a certain pace looking ahead because if you...

**Minwoo:** Exactly. Yeah. you you you don't make those you don't make those components and you end up needing them then we'd be here I wish we hadn't gambled that LLM would get so good.

**Adam:** Yeah. And it makes actually what you're saying echoes a lot of our patching team. So, I'll be sure to introduce you to them because in Vegas because there's a lot of I think they too are sad by by how much effort was having to keep being undone and redone and like the techniques that used to work on the old models don't work on the new models and actually decrease performance. You have to undo some of the hacks that you were doing earlier and it's just like kind of kind of like a you're like a treadmill. You're just trying to like chase that good patch but it's hard.

**Minwoo:** Yeah, exactly.

## CTF Comparison and Future Predictions

**Adam:** Question I have real quick for hanqing95 over here. How is the AIC in your mind because you've been involved in some top CTF teams, top CTFs. How's the AIC compared to a CTF?

**Hanqing:** I actually feel like it is the next the next version of CGC. So compared to the CTF we you know in the CTF we usually focus on how to exploit the bugs, how to exploit the low quality bugs actually but for this competition we are more focusing on the bug finding. It is pretty different from the CTF. You you know in the CTF we usually usually do the reverse engineering by using human. In this case in the competition we need to replicate ourselves to to make our agents our system to mimic the ideas in our mind that's also the philosophy for us to design the system to replicate the security researchers into the agents.

**Adam:** So idea a CTF where you your only interface is the LLM. You have to tell the LLM, you have multiple LLMs, maybe agents, etc., etc., but you just have to guide the LLM. It I got this idea when you said replicate our live CTF.

**Hanqing:** Yeah, that could be interesting. Yes, we we actually we already have some try in our internal CTF in Georgia Tech. We have we also have a CTF course and we already run our system along the competition the internal CTF in our university. So we we again it's similar to what Minwoo mentioned before a year ago we create the CTF agent that writing exploit for every single challenges that we have for the class so that we can use as a you know benchmark and CI and whatnot didn't work very well this is achieve like four weeks of competition but after that is complete failure but now everything changed so I think we it's a good time for us to try again and start building.

**Taesoo:** The one of the prediction that I have is there the one prediction from AI community there will be some one person 1 billion market cap company will appear. Similar here similar thing will happen in security community there's a one person CTF player win the DEFCON CTF with the help of LLM or AI in general I think truly less than eight people person with the help of AI they're going to they're going to win the competition sometime soon and their hand will be LLM heart. So they're the manager of LLM or worker of LLM. This LLM in a way. Hey, do do this for me. So LLM can figure it out how how the flag.

**Adam:** Wow. Yeah, that'll be super interesting. I mean, we've already had like go going back to kind of what Jan was alluding to. I mean there's a history I don't know DEFCON finals but definitely quals of people qualifying as a single CTF like a single person team so it'd be super interesting.

**Taesoo:** The challenge is more in the organizer side because now everyone has as a LLM access like previous like it's not even challenging anymore I think bar for most of every user is so high thanks to LLM like how to deal with this difficult...

**Adam:** There was a pseudo random number generator reversing challenge that we were working on for a while. I I wasn't involved in this but the team was. And eventually they kind of hit some wall. They pasted the whole code into an LLM with the seed that the server gave them when they connect or not the seed the random number that the server gave them when they connected and said what's the next random number and it was correct.

**Taesoo:** Wow. Wow. It was insane. It could have been blind luck, but it was insane.

## Conclusion

**Adam:** Yeah. All right. Well, on that note, we're going to have to say goodbye to our guests. Hey, thank you team Atlanta for joining us today. I think we had a great conversation. We learned I know I learned a lot from from you all. So, I'm Adam D. You can find me online at Adam Dupe. He's Zardis. You can find him online at Zardis. We're CTF radio. You can find us online at ctfradio o's not zeros. You can send us questions through email at ctfr@gmail.com and your questions might end up on a future episode of CTF radio. Goodbye.

**Team Atlanta:** Thanks. Thank you.