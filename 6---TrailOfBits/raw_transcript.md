Hello everyone, welcome to CTF radio. We're here in our six out of seven AIC team interviews. So Yan, it's been uh it's been quite a long uh and a long road. How are you feeling? I am feeling great. One, because I am running on zero sleep, which means I'm on a complete uh exhaustion high with a lot of coffee. And two, because uh it is super cool hearing from every team over and over and over how hard they went for the competition, right? It's one thing to have millions and millions of dollars on the line and and you know someone at some point swoops in and grabs the whole bag, but uh hearing how hard people have been pushing um has been super cool and also helps frame our own efforts um put them into perspective too. Um it's it's we've been uh well we I mean you know the the whole team has been pushing uh forward for years and it's uh really interesting to think that this whole time we've been going it it felt like everyone's going it alone but you know it's it's it's it's some sort of a there's six other teams for it for sure suffering in parallel you just couldn't see them right yeah and and actually on that note a bit of of backtory story that I probably wouldn't mention, but I think it's it's relevant here is uh our our next guests, they were one of the first people to respond. Yes, we're super happy to do the interviews, but we're taking vacation, so let's do it later. And that I think perfectly ties in with the, you know, effort that it's clear that teams kind of put into this event is after the submission, you know, people need time to to recharge. Um, and it's it's actually really brilliant after the submission, but before the results come out. So you don't even have to be like over the moon or or or or completely dead. You're just like you don't have to care. Exactly. Yeah. For the for the um for the vacation time. Cool. So we have with us today a Trail of Bits and we have uh Michael and Evan here. So uh Michael, you want to go first and introduce yourselves to the great people out there? Yeah, sure. So I'll start up with our with our company. If you're not familiar with us, Trail of Bits um is a cyber security services company. Um we do a lot of commercial work, but um I'm part of our research and engineering uh practice and we do a lot of work for DARPA. So um we do a lot of research for government clients, including like ARPA, which is now kind of a co-sponsor slash um co-runner of the AI cyber challenge. Uh we also do some some work for big tech and and commercial clients in in other parts. So we're about 120 people. Um I personally am a principal security engineer is is my title. I lead up our t our uh company's a IML security research team. So kind of two flavors of that. One flavor is applying a IML to security problems which is you know right down the alley for AIC and then we also do a little bit of work um on securing like a IML pipelines mostly with like supply chain type stuff. So I've been with a company for like you know three or four years now. Wow. Cool. That's awesome. Evan, what about you? Uh yeah, hey, uh I'm Evan Downing. I'm a senior security engineer at Trell Bits. I joined uh last year uh as part of the R&D group uh under Michael and I've been part of the AICC team uh since I entered in the uh semi-finals. So, it's been uh fun to to work with them and grow with them. And um yeah, particularly I was in charge of uh you know, one component of the of the CRS, but also we had our hands in other other areas of it too. and um yeah, just en enjoying uh enjoying working and uh and contributing as I can. Awesome. Wait, wait, so you joined last you joined did you join right before the like qualification and did you know that you'd be doing this? Like I did. Yeah. So I joined April of 2024. So we already knew we were going to go into it. So it was right when we were set up to actually get all the semi-finals rules and start start hitting the ground sprinting. So that was part of that. Great. So they didn't just pop that on you. they like you started and then the next day after orientation they're like, "Oh, by the way, you're going to help out with this uh giant AIC." No, I mean it was it was planned. It was introduced to me that I might, you know, obviously help out with that when I was doing my interviews. But uh yeah, so it wasn't it was no surprises necessarily, but it was definitely right up my my alley and uh right right up my experience level, so was comfortable uh getting in that position. Awesome. Very cool. Well, yeah, thank you both for being here today. We're really excited to kind of hear from all the teams and specifically to kind of, you know, today dig into to your team. So, can you give us maybe some background on the team about roughly like how many people, what's the size of the team? I think Michael gave us a great uh overview of kind of the the trail of bits as a company, but inside of there, how much were kind of dedicated to AIC? Yeah, sure. So our ourCC team is entirely composed of uh people who work at Trailab Bits um and also people who worked at Trailab Bits before uh the competition got started. I think at one point we kind of briefly entertained the idea of bringing somebody on to assist um but we ended up staffing it entirely uh internally. So um you know from the beginning all the way to the end we've had about 10 people uh between eight and 10 people working on the project at any given time on a meaningful basis. Um we had a large number of people who kind of helped out in the beginning with like ideiation and kind of you know running some stuff to ground um and helping us out with some various kind of like you know rubber meets the road type things but this was all before we actually knew really even how the competition was going to be structured. So that was back when there's this huge wide open possibility of what this actually could be and then when it came down to what it actually became uh we really had a team of about um about 10 people. Um, of those 10, uh, we typically had about eight at any given time, like actually working on the CRS. And we we've maintained throughout the entire, um, project, a twoerson, I kind of call them our red team, um, whose job was to basically make challenges that would try to make our CRS for, you know, for lack of a better term, puke, and, um, and try to challenge us that way. Yeah. Big shout out to uh Brad Swain and Aleandre Gario on our team who who did the hard part of uh of making us work extra hard to to try and live up to a standard. You know, that was a little bit beyond the the exemplar programs we got from DARPO. Yeah, that's great. I mean, I think that uh gets into a lot of kind of like AI one of the main focus that I see is on benchmarking, right? Like how do we create good representative benchmarks? Um yeah, Yan, go ahead. Reading between the lines, this uh means that to support a red team, you would have had to have a runnable CRS pretty early on and continuously. Yeah. And we the way we do things at at Trail of Bits, this is really for all of our research projects. We we we do like an MVP style development. So we make the absolute barest bones possible minimum viable product that does end to end whatever like this this like impossible to do thing that we proposed is. Uh so even if it you know it means it really only supports like one exemplar program or a very tight narrow definition of that we started off with that and then we started adding support for all the various um aspects of it. So now admittedly there was some lead time. We kind of knew what exemplar programs were going to look like but DARPA didn't give us very many. So we we kind of sent um you know Brad and Alisandre out with this mission of like go find hard stuff that's going to make us break um so that we don't train to you know specifically the test that DARP has given us and then you know we find out the test is a lot harder and we and you know we we stumble. So um yeah and then as we kind of learn more about you know what the actual system was going to look like, how long the the the cyber reasoning systems were going to run, what kind of resources we had, we would kind of adjust fire on um on like what kind of challenges we were trying to create. But but yeah, absolutely to Adam to your point, it was it's a data set problem. You know, we got we got an N equals two or three when it comes to data set size for example programs from DARPA in the semis. We got more in the finals, but it was still, you know, you could count them on two hands. Um so so yeah, if you're unless you want to if you want to stretch the capabilities of your subs, you can't you can't just go with like two or three examples and hope that you do okay. Um it actually honestly we kind of stumbled quite a bit in the semi-finals when it came to Java challenges because um we only got like one Java challenge ahead of time and um you know we could process it. Everything looked good on our side but we didn't have time to make a lot of Java challenges on our own and then you know when we get to the semi-finals our our cyber reasoning system um it really struggled. I don't think we got any points on any of the Java stuff. And um we really think it was probably more due to the fact that like it wasn't due to the fact that the CS was capable because it really performed quite well on the C and C++ challenges, but we think something went wrong with like the Java class path or the types of uh CWEs that were being examined there were things that we didn't really tune our system for. So we actually never really found out what actually happened with it because we didn't get a lot of information back. So, uh, we do better on the Java challenges now and we, you know, we had Ryan and Allesandre find a lot more Java challenges the second time around. That's for sure. That's great. Okay. So, yeah, before we dig too much into the system, I want to kind of roll back a little bit. And so, you guys were one of the small business prize winners. So, the AIC must have kind of been on your company radar for a while, right? Oh, yeah. That's that's for sure. Like I said, um you know, so as a company about 30% of our company's revenue comes from long-term research contracts with the US government. So DARPA, ARPA, sometimes ARPA E, IRPA, some of those other star ARPAS that are out there. But for the large part, we we we do a lot of our research about 30% of our revenue comes from uh research programs. Um and within the research and engineering division of of um of Trail of Bits, it's basically flipped. about 70% within that division comes from from DARPA. So we do most of the DARPA. So they're our primary sponsor. Um so we're we're hyper aware of the things they put out. This has been something you know a research area of mine um that you know was was a big part of a PhD that never got finished and then also a big part of you know what I've been interested in researching for the last five years. So personally, I was pretty hyper aware of this, but also, you know, more largely before my time at the company, Trail of Bits competed in the CGC, didn't make it to the semi-finals due to kind of an unknown element of the scoring at the time that ended up kind of um costing them a spot in the top seven. Uh but they teamed up and went to the finals with another team. Uh they ended up not placing uh there. Um, yeah. So, Trilobits was heavily involved in the Cyber Grand Challenge and this being kind of a spiritual successor to that. It was something that, you know, the minute it got announced, we were all we were all we were all we were we were on the DRO list, so to speak. Very cool. Yeah. That's awesome. So then I'm curious if you can and of course if you like we all understand your company you may not be able to say things but I'm curious how the requirement to open source the systems kind of either altered your approach to participating or was it not a barrier at all um like you know when that when especially with the because I think that was from the from the jump like as part of the small business prize you needed to guarantee that you'd open source it and and all of that. Yeah, it really was no problem for us at all. Um, to the maximum degree possible. Um, and this is companywide. This isn't just with research projects. Trailbl tries to open source everything we create. Um, that's one of the really great things about working for a services company, not a product company. Um, Trail of Bits has made successful products in the past, but they get spun out as separate entities and we remain like a services company. So we don't have any you know financial um incentives to you know engage in this like heavy productization. Now that's not to say that our that buttercup or CRS might not get productized after this but um realistically I mean a part of the reason why Trail Bits is well known in the community is because we open source our tools. Tools like Mixmma have been really popular. Um a half dozen tools that Evan and I have created over the years either at Georgia Tech or at Trail of Bits have been made open source. So, um, we really try to put everything out there. So, this was it was really no factor for us at all. Um, I think it was kind of a consideration in the finals when it came to like open sourcing AI models, um, in our decision about whether or not we were going to try and fine-tune or create our own model. Interesting. Um, so I think that was a little bit of a of a of a consideration there. Um, but realistically when it came to open sourcing the CRS, I mean, we were going to do it anyway. That's great. Okay. Awesome. Yeah. Like, you know, it's it it kind of makes sense. It's like a selection function, right? Right. I'm sure we never would talk to the teams that would have submitted except for that uh open- source requirement. So, uh that's great. Cool. So, maybe now can you you mentioned uh Buttercup. So, can you give us kind of the highlevel overview of how the CRS works? Kind of the 10,000 foot view. Yeah, sure. So, so our our butter our CRS is called Buttercup. Um and um based on how the competition ended up being structured, it really heavily influenced our design. So Buttercup's basically a large distributed system that works and functions basically in a pipeline. Um so you know early on when we first saw the original structure of the competition, we saw there's basically like three things that you um have to do to to win the competition. You have to discover vulnerabilities. You have to characterize or or contextualize vulnerabilities. you have to patch them and then you know kind of like item three and a half is you just have to like kind of manage like the competition or the gamesmanship of this but it's not really like a like a major like task. So um so as most people know um and I'm not going to get deep into the rules here but you know the competition requires us to find a proof of vulnerability. So we've got to find a crashing test case um and you have to do that before you can patch. That constraint was loosened a little bit for the finals but more or less you have to find a vulnerability then you have to patch it. So you know A follows you know A A precedes B, B precedes submission. So we kind of have everything um set up in that way. So we we scaled different aspects of our system or different components of the system and replicated you know more nodes for um for certain tasks than others based on like how that pipeline worked. For example, we have lots and lots of fuzzing nodes, a relatively or comparatively fewer but still a lot of patching nodes. Um that's because you got to find the vulnerabilities first and then patch them. So um at a high level view um you know we the CRS receives um or communicates with the competition infrastructure with an orchestrator. Um the orchestrator is where all of our scoring strategy and gamesmanship lives. That's kind of component 3.5 there. Um then the uh orchestrator then sends or coordinates the actions of the other other groups of nodes. One of which is vulnerability discovery which um you know more broadly kind of contains two subcomponents that um fuzz the target fuzz the available um the available fuzzing harnesses. We use uh large language models to help us generate uh seed inputs that are either likely to trigger certain types of vulnerabilities help us explore more of the code base um or otherwise just kind of generate interesting behavior. Um so that that vulnerability discovery component is the first step of the pipeline. The second step of the pipeline is this contextualization process. Um which uh this is what Evan was in charge of at the I'll let him describe it at the lower level but at the um at the high level um it is a collection of um static analysis and other um kind of automation type tools to feed information to both the C generation and the patching system which is where we also use a lot of large language model support. Um so this provides additional context that is necessary for solving these problems in with large language models. U so once we get a vulnerability from the uh vulnerability discovery component and once we've contextualize it, it goes off to the patcher. The patcher generates candidate patches uh figures out which ones work, which ones don't clobber um existing functionality. Um and then uh once it validates one of those, it submits uh or it sends that back to the orchestrator to submit that. Um there's a little bit more in there with the idea of like the serif broadcast that I kind of didn't mention. Most of that logic uh actually lives within the orchestration component. It we actually didn't handle the serif uh broadcast very much if at all in our in our CRS. Nice. Okay, great. But then so you have so this orchestrator is this like a I guess a little bit more detail. What kind of languages are the components written in different languages or does everything kind of standardized? Maybe we can kind of dive in a little bit deeper. Yeah, sure. So, our our our original CRS that we made for the semi-finals, um it had a little bit of Rust in it. Our vulnerability discovery component was written in Rust. Um but for the most part, you know, I'm sure you've heard this before. You everything's in Python for the most part. Um so, uh you know, that was like a kind of common language, easy to implement, uh easy to test, a lot of that kind of stuff. So, so yeah, it's mostly written in Python. In the for the finals, um it's it's entirely written in Python. um at least the code that we wrote, anything that we've, you know, um vendored or included as a dependency, it's it's in whatever language it was in, but anything we wrote was was more or less in Python. So the orchestrator is probably the only part of the system that is I mean there's probably like a like some other random node somewhere like a task server or something. But for more more or less the the orchestrator is the only um single choke point in the system. Every other portion of the pipeline is highly replicated. Um this is mainly for uptime and for for throughput when you know some of these later rounds have quite the quite the quite the scale that you have to handle when it comes to you know target programs. Um so so yeah the orchestrator is is um is probably the only place where we we only have one of them. Um, and it mostly just communicates with message cues to the other components, but it does also house all of our strategy for like how we're scoring, how we're deciding which POVs to submit, which patches to submit, whether or not to attach a serif to them, bundling, all of that stuff happens there, which is great because we we really practice a lot of separation of concerns to try and make this problem like tractable. And did that help with the testability as well? You mentioned, you know, making sure like testing with red teaming and stuff. Did that allow you to then test kind of each component separately while also testing the whole system? Yeah, absolutely. That design was was intentional. One of the great things about Trail of Bits and and the wide variety of work we do here is that we didn't really have to look outside of our our tent to find, you know, talented software engineers who've worked on systems that need a lot of uptime and need to be reliable. we didn't have to look anywhere else to find researchers who were good with AI or researchers who were good with machine learning, researchers who were good with um you know software analysis. We kind of had everybody under under one roof. So one of the things we knew would be an advantage for us going into this was um as an established company you know we with with with personnel to actually do this. We knew that we could always put more work than I think a lot of our competitors could into maintaining reliability, robustness, uptime and all these like kind of engineering aspects. So uh even though this is you know a competition to you know drive forward you know the state of research we did an embarrassingly large amount of engineering into the system which included you know we we had basically you know some version of CI/CD that was running um one of the I think it was lib XML 2 one of the one of the targets that basically every time somebody pushed a commit um to GitHub that we we ran from end to end fired up the entire CRS uh towards the end we had we had to turn that off because we were running out of credits. But for for a long time, um you know, even if you were even if you just changed comments or updated a configuration file, it ran everything. So, we uh we kept a close eye on, you know, whether or not we were breaking things as we go because like I said, everybody um was working on different components. We're working at a rapid pace. Everything's got to get plugged in. Um and the integration, we didn't want that to be an event or to be a headache on its own. So, we we largely automated that. That's great. H how much did each run cost? It didn't cost a whole lot. Um so we actually got a shout out from this from Andrew Carney in the semi-finals. He said we were like the most judicious use of the of the large language model um among all the teams. I think uh I think he told I don't know it wasn't him. Somebody one of the one of the people the organizers like told us that for one of the challenge projects that that we use like $14 in LLM credits for for processing. Wow. and and your final submission and your call submission were similar on this. Uh yeah, we were so we we we eventually tuned and scaled the system to use more resources because they were available. Um but in the um first exhibition round um when we got our report back on how much we used, I think we had a budget that was like something like $5,000 or $3,000. It was it was several thousand. we use $162 and we found and patched all four vulnerabilities that they that they put out. So, um yeah, we were we were quite um uh were quite efficient and also the compute was more or less in the same same boat. Um so, we we actually didn't really use a ton of resources. We started getting to the point at the end we're like, okay, we actually have to use these things otherwise we're leaving stuff on the table. So, we're like so we actually had these kind of weird problems in the last couple of weeks. were like, can we increase the number of fuzzing nodes by like four times and and this thing will all fall apart or not? And yeah, we that was a question we had to answer. Um so yeah, most of our CI runs, they didn't actually cost us all that much because we were always really um kind of resource conscious uh when it came to uh when it came to like how we designed and how we built the CRS and how we used AI, how we used compute. That's great. We were actually talking I think Lacrosse had a and SIFT had a similar uh usage of LLMs and they were kind of advocating for a they're hoping that maybe DARPA will do some kind of award for like most efficient uh CRS in terms of bugs found versus money spent, right? I think there actually is an interesting like decision space in there of of finding the different CRS's and and what what resources they can use. So, I I was talking to someone today. Um, I mentioned early on, I think throwing around strategies and and various approaches, we uh calculated how much it would cost to just like put every line of the Linux kernel through GPT, right? Um, and it was like $10,000 or something at the time. Um, and I was talking to someone about this today and they said, "Well, you know, I mean, depending on what kind of bug you get, $10,000, that's not a bad investment." So, maybe you've been thinking about this all all wrong. You know, it's about uh throwing the maximum money in. But I think that there's this, you know, uh reduced returns that happens eventually. Yeah, for sure. The point of diminishing returns is really real with applied AI particularly now now that applied AI to security problems looks very much like you know you're you're paying a third party for the AI the AI is incredibly compute um intensive um you know it's one of the things we kind of considered at the end of this which was um you know we are going to open source this we are going to try and share this with people it doesn't really do anyone a lot of good if we make a system that requires $50,000 um in compute and you $20,000 in LM credits to find you a couple of bugs. Um, you know, it probably exceed most of the bug bounties you would get if it was like that kind of situation. Um, you know, value and risk to the business is not always that high. Um, so really we wanted something that people could use because I mean if you think about it, you know, a good portion of of AICC's infrastructure is based on OSS buzz. So if you require like big tech level organization to run this system at the end of this, the only people that are going to benefit are the big tech organizations because they will just they'll take Buttercup's patching component and slap it onto OSS Fuzz and then now OSS Fuzz Plus or whatever they want to call it does a great job of submitting patches. you know we you know we helped you know you through Google or through whoever you know substitute whatever organization you know through them we've now helped the open source ecosystem but um that doesn't help when you want to port this to other places that also need security services like embedded systems hospital infrastructure they don't have they don't have the money to go stand up their own Google and data centers and all this stuff to run this stuff um it was it was a big goal and a big design decision for us to say like yeah we're going to use a lot of resources now to try and win this competition but wouldn't it be great afterward if we could tune this down, run some experiments and say, you know, yeah, we spent 100% of the budget in the competition, but we got similar results by by tuning everything down and you spending maybe 20% of the cost. So, that's an interesting research question for us. Almost like distilling a model down in some sense, right? It's like distilling a CRS down into something that's more tractable. Do you have any any kind of preliminary ideas based on your testing? It sounds like you you've definitely done an above average amount of testing compared to the the typical uh competitor in this Yan. I don't know about that. We had a uh we had a button that anyone on our team could press to run a full run on any target and they were like, "Yeah, I click this button. I spent $250 to run this." Definitely true. Considering how much I've complained about the testability of of artificial in our podcast, it was shocking. Not saying that was the most efficient way of doing that, but uh you know when when the m the money's not coming out of their pocket, they were very happy to click that button. Yeah. Yeah. The people had the button. I mean that was certainly a factor for us too. We ran a bunch of almost every week um for the last two months of the competition we were running a exhibition round scale version with of of Buttercup um mainly to shake loose um reliability type bugs that you can't get when you know like it was great that we had CI/CD that was you know running every time you push a commit and would make sure that the toy example would would run you know that would cost us like you know a couple hundred bucks at absolute most um but we would spend you when we spend our LM credits and when we spend our um compute credits. It was like 10 12,000 at a time. We got to the point where our very last run right before we submitted, I was like actually like watching our Azure update as like at whatever cadence hit updates. I was like, "Ah, turn it off. Turn it off. We hit 100K." So, yeah, we maxed out credit card at one point like I we hit the max and I had to keep an eye on it and just feeding money into that card because the claw credit and the Azure like hold were just going way over. It was pretty wild. But uh but yeah, that's the you know the there's a definitely an interesting trade-off there between the playing the game and and creating a system that lives beyond that's usable by a a non- tech giant. I think that's kind of a great point. And I also I I I love that that concept also with integrated CI. It it it all kind of goes into this, you know, life beyond AIC, right? If you have a a CRS that's impossible to deploy and untested and uh has you know unclear components and connections uh it's much trickier. Now you mentioned you had your custom written orchestrator. Um did it were the like the glue pieces between that and the components are those off the shelf were those custom written? uh we use like reddis cues and reddis databases to handle most of our persistence. So we relied on on on kates to do most of like the uptime management. So it was like set really aggressively that if anything went down it would just you know kill it and restart it. Um so you know we persisted everything to disk um that was like a meaningful artifact. Um and then so basically all of our individual components when they would wake up whether it's the first time they were started or they'd been restarted um they knew just to go look in a particular queue for a task to be completed and we really relied on that for um for scaling the system up to large number of nodes are all performing the same task to different nodes that are performing different stages of the task and then um really when it came to you know like with the functions of the orchestrator it basically did the same thing it's just there was only one of them because we needed one central place to decide what we were going to submit and this worked out great. Yeah. Yeah. For the most part. I mean, we had our problems. We I mean, so I don't think we told anybody this, but like for the second exhibition round, we found like eight vulnerabilities and patched like six things and we all thought that we were we thought we were cooked. We're like, "Oh my gosh, we're just stumbling. This is awful." We found out like six hours in that um the uh the way we were creating file names for POVs was um putting too much information into the file name. We were exceeding the size of the the or the the length maximum length available for file names and that was like crashing nodes hard like they were. So we were like we went from basically like you know basically we did the best in the first exhibition round and it was only like two challenges, four vulnerabilities, a very small scale but we did the best. You that's good. You don't want to you don't want to do the worst. Um if you got to pick a place to be after the after like the the test run you want to you pick first. And the second one it was like way down the roller coaster like oh man we're we're going to get yelled at so hard like this is going to be awful. And then we figured out it was a bug. We're like oh cool we can fix this. and then we ran it and then you know performed much much better um the the second times or in the sec or in our own run of of exhibition round two. So I mean yeah we we had our fair share of broke stuff that's for sure. Yeah but generally speaking it doesn't sound like you regret the uh those decisions. No, I mean when it came to um I mean I think you could have certainly for the semi-finals and certainly through the first exhibition round you could afford to keep things pretty small scale and not really write with any scalability in mind but the second you got to exhibition round two three in the final scored round if you didn't have anything built in for scalability. You were going to spend the last two months of the competition instead of trying to improve your score you were going to spend it fixing scalability issues. So, we built this thing to scale in the first place. Um, which is, uh, it meant it took longer for us to get to a place where we had something runnable and testable. Um, so we were, you know, like I said, for the last two two months of the competition, we were doing a lot of these full scale runs. It's because we had a lot of our credit saved up. We didn't really spend a lot um, January through March. Uh, but once we hit April, we were spending a lot, but it was also because we were testing at scale almost immediately. You might be the first team we've talked to that has no qualms about the plumbing of their CRS. Uh I mean Yeah. Oh yeah. Look, our our issue log that thankfully is that thankfully got stuck you don't have to look at or think about anymore. Yeah. Yeah. That thing was full of a bunch of stuff that we didn't like or that we wish we could have fixed. Um but uh yeah, for the most part um I mean we had to test it a lot and we had to fix a lot of issues with it. But for the most part, we really didn't have issues with um with dealing with scalability, but like I said, we also like every question when we we made our when we made our diagram for Buttercup version 2.0, it was like is this going to scale? And um that's part of the reason why we were really um constrained with our resource usage. We didn't know what the final round, what the budgets was were going to be. I mean, the budgets in the semi-finals were like astronomically small. I got $100 in LM credits and like $500 in compute. You only had to run for four hours. So, I mean, you know, it's probably commensurate with that, but we had no idea going into the finals like is it going to be more of that or is it going to be what it actually turned out to be, which is, you know, six figure budgets, five figure budgets. So, we we knew from the beginning that like if we're going to have to run this a lot, we need to use very few resources per vulnerability. We uh might have to process a lot of targets. So we can't we need to be able to rely on replication to scale and not um just throwing hardware at it because you mean you can only get you know like the processes are only so fast like they they've been killed by you know they've been killed by a wall of diminishing returns and you can only you have to parallelize if you want to scale. Yeah. Wow. Okay. Cool. Great. So maybe then we can dig a little deeper into the CRS. So can you uh walk us through what are the kind of the steps that happens with the orchestrator and all the components when a new repo arrives in buttercup? Wait, first sorry before that what does buttercup stand for? Is it just a name or is it an acronym? I've been asked this like five times and I promise you I wish there was a good story for why we named it Buttercup. We actually like it was just like made up. So human made up or AI made up? No, human made up. Human made up. Um, so here's the best I can give you for a story as to how the buttercup name came around. Um, when I when I along with um Ian Smith, who's no longer uh with us, I say that at Trail of Bits, he's not dead, but he's dead to us because he went I was like, "Wow, this is getting really dark." No, no, he's not dead, but he's dead to us because he left the company and went to a competitor. But um um but he and I we wrote the original concept for the the original concept white paper. And back then, um, my kids were really into the movie Wall-E, so I named it Patchy. So, if you look back at our original like concept white paper, if it ever gets published or whatever, it says the name of our CRS is Patchy. Um, but as time went on, um, it wasn't terribly like marketable as a name. And we also wanted to avoid names that like anthropomorphized the like the AI because that was becoming you know one of these kind of like ethical questions in the AI community is like should we give these things human names because it makes them kind of I don't know it's it sets a weird set of expectations move away from that. So we we we talked about it internally to the company and um our CEO Dan I think came up with the name or was the one to suggest it and I mean it helped me when it came to titling some blog posts and some talks I could use the buckle up buttercup you know idiom but but beyond that like it didn't really come from anywhere we just wanted something that was simple easy to remember um you know wasn't like a like an explicitly human thing I wish I could say we were all like big fans of the princess bride we are but that had nothing to do with naming it Buttercup say It's like a good horse. Like maybe a CRS and a horse is a good metaphor. Like it's intelligent, but it's maybe not as intelligent as a human. It can run faster than a human. But uh yeah. Yeah. Honestly, I was just thrilled that we didn't like follow like the other like kind of like well rudded um I don't know like tech naming schemes like calling it Trail of Bits Plus or or Patchify or something like that. So yeah, I was pretty I'm pretty happy with the name. It is pretty easy to remember. So yeah. No, it's great. I really like it. Cool. Okay, so then back to Buttercup's details. So what are the steps that happen when a new repo arrives at Buttercup? Yeah, I mean I imagine this will probably be pretty similar to our our other competitors. Um mainly just because it's kind of how the competition is structured, but um when the competition infrastructure sends us a task, um the orchestrator is the endpoint that receives it. It's it's where the the API um is implemented that lives. Um, so you know, we pull down the competition, um, the competition repository. Um, we build it and then basically we start fuzzing it. Um, so we build it with a couple of different sanitizers, all the ones that were basically in scope for the competition for lib fuzzer and then also for Jazzer. Um, so we build different versions of it. Um, each equipped with the sanitizers. Um, and then we start fuzzing it in parallel. Um, our LLM driven C generation process is also running. It's adding to the corpus new inputs that will, like I said before, either help it explore and get better code coverage for the code reachable from the particular harness. Um, that that fuzzer is responsible for fuzzing. Um, and it also generates inputs that are likely to trigger certain classes of vulnerabilities that mutational fuzzing isn't necessarily great at generating. So, think of these as like grammar-based type vulnerabilities. Like a good example is like a SQL injection. Um, it's very rare that a mutational fuzzer, you know, given just a random, you know, seed of a clump of bytes is going to stumble upon what is mostly valid SQL with a with a correct terminating character and then the, you know, the correct grammar to start some other um some other query that's malicious. So, we asked the large language model to help us generate inputs that were likely to trigger like SQL injection, path traversal, other types of specific vulnerabilities where and is that given the target? So, what are you giving to the LLMs to make that? And is the LLM just is is the LLM spitting out a seed or is it spitting out like Python code that generates a seed? It's the latter. It's Python code that generates a seed. It's it's actually quite bad at at at path reachability and and code reachability problems. Um, it just requires a lot of reasoning about the dynamic nature of a program that large language models just they they aren't architected to do and are never going to be good at. Um, but they can generate code. So we asked it to generate small Python programs we can run in a sandbox that generate these inputs. Um and and it was uh surprisingly good at at doing that. Um this one area I was kind of my expectations were were exceeded by a lot in terms of how effective uh that strategy was. Um so so yeah the CGEN um bots they are busy generating seed inputs um to help the fuzzers um be more effective. Um we do this for two reasons. uh one we tried to convince the people who ran AIC to extend the the processing windows because u one of the qualms we had with the semi-finals is that you know if you want a POV you pretty much you have to fuzz some way shape or form you might not call it fuzzing but really what you're doing is fuzzing. Um can you elaborate on that? like what might you be not calling so so let's say I let's say I get rid of all the mutational fuzzing piece and I say LLM generate inputs that are going to that are going to they're going to trigger a vulnerability for it to be a POV and for you to be reasonably assured that you should submit it you have to validate it which means you have to run it so really what you've done is you've replaced the mutational fuzzing engine with a large language model that generates the inputs so you've made a less efficient fuzzer so no matter what because you have to because you need a POV and because you have to validate it before you submit it you are doing fuzzing like there's just no way around it. So, um, so, so yeah, that's I don't know. That's my that's my philosophical stance. I'm sure someone out there would love to debate me on it, but I It's interesting. Yeah, I like that there's even this bit of randomness involved as the temperature or whatnot, you know, like there it is essentially a a custom mutation engine. I mean, it really is. Um, yeah, it really is. Um, yeah. Yeah. So, I guess getting back to the original question. So, so yeah, the the CGN bots, they're they're trying to improve fuzzing and it's for a couple of reasons. One is to um find vulnerabilities that are harder to reach, stumble upon by mutational fuzzing. I kind of covered that, but two, it's also to meet the the time horizon. Uh we tried to convince them in the semi-finals to give us a lot more time to run these things because um anytime you talk about doing something with fuzzing, you're usually talking on the amount of time you're allowing the fuzzer to run on a given harness usually in like orders of like days. Like 24 to 48 hours is like what you'll see in the literature and we're talking the the you know the the point of four hours and it's like okay I built this system and you're going to you're going to you're going to rate it against other systems and you're going to tell me I wasn't in the top seven or I was. It's like well it's a coin flip game on how good the mutational fuzzing engine happened to stumble upon the vulnerability and for it's not really a true measure of the system. So um in some of the earlier exhibition rounds they had longer windows but I think that kind of pushed them back down. So um in order to meet the time horizons for for doing this we had to fuzz or we needed to get higher coverage um on the fuzzing harnesses faster. So that's why we also include a portion of our um of our C generation which um was uh Ronald HSN was was the member of our team that was in charge and and owned that tool and did a did a like absolutely spectacular job with it. Um yeah, it really it really helped us find those vulnerabilities within those within that time frame and then also helped us find vulnerabilities that are harder to stumble upon with mutational fuzzing. Awesome thing. And and so this was this it it generated synthesizer seed synthesizer programs but it didn't it it kind of implicitly built in grammarss just in by virtue of how it built the program. It didn't like for example shellfish generated grammarss and then like had all of this like combination of grammarss and generational fuzzer from there. Um, but you you you kind of basically you had the CRS do or the LLM doing that implicitly in its head and then generating the grammar based generation program basically. Yeah, we didn't want to constrain the large language model and how it would go about doing that. We really relied on it pulling something out of its training data which is probably, you know, some input generator for SQL injection vulnerabilities. Um, you know, so we we we wanted to keep it as broad as possible because the more information we gave it, the more likely it was to hone in on just one solution. And you know, we running we're running these C generator bots um uh many many times with different configurations asking them to focus on different CWEs. So we wanted them to be and sometimes you know the same CWE has like two or three C generator bots for the same challenge. We wanted them to come up with different different possible um solutions. So with that the temperature setting on the large language model, it's possible that you know it hones in on one implementation that it that it that it found in it training data from you know one one person who made this tool and then some other person who made a similar tool and another person made a similar tool. All of those would eventually kind of be brought to bear you know through through code synthesis and and seed synthesis. Very cool. All right. So then you get a POV, right? And then what happens after that? Yeah. So the actual fuzzing part um was more interesting in the semi-finals but is very uninteresting in the in in this in the finals and I I suspect this is probably the same with most everybody where people just used oss f or cluster fuzz and and the oss fuzz infrastructure that was provided. So yeah when we get the POV um our orchestrator gets it back and it starts um looking at it and it decides whether or not to submit it. Um there's a little bit of dduplication that goes with this because um for the most part the competition really didn't disincentivize you from submitting duplicate POVs. There was no penalty for it and also you might be able to clobber some other um teams uh patch if you submitted a lot of POVs. But um we had an an interest on our in our own system to keep the number of POVs down because it meant there was less time spent patching. Um so we had a we we used basically the same infrastructure that the competition organizers were using to dduplicate uh vulnerabilities. Um this is based on the stack trace based on uh like instrumentation traces and we we also dduplicated across um sanitizers because the same bug will trigger multiple sanitizers. So we dduplicated there. So ultimately we kept the number of vulnerabilities down that were going to process but we were pretty liberal with um with if something did come out but look similar we were pretty liberal with with submitting it. So from there the vulnerabilities now go to a patcher. The patcher is supported by our contextualizer. Um the contextualizer also supports the um supports the the C generation mechanism but it it was really like conceived to support the patcher. So for that I'm going to hand it over to Evan to talk in detail about some of the stuff that we did there. Um Evan was the owner for this component throughout the entire competition semi-finals and finals. So um we had a couple people um on the team not able to continue with us and some that kind of came on new for the finals. Evan has been subjecting himself to this abuse for the entire two years and deserves deserves deserves full credit for this um good or bad but I'll I'll let him take it from here. Awesome. And just the contextualizer component is what you worked on or the patching component? Contextualizer component. Okay. Very cool. Awesome. Yeah. So, um, so as Michael said, after you get a POV, what do you do with it? You give it to a patcher. So the idea is that the patcher would take some information from the POV, the the stack trace, some sort of artifacts from the crash itself, maybe the bite input, um, and try to figure out, okay, where where in the code does it does this, you know, issue manifest, where's the root cause, and then how do I patch it? Um, so in order to get this type of information, uh, we decided to create a contextualization component that could be queried. So the the the high level overview of the contextualization component is just a a query mechanism a library that um an LLM can use as a tool to query various information about um the software structure of the program's information. So things like you know if it finds that in the stack trace it calls a certain function or crosses a certain a certain line in a certain function it can query the contextualizer and say okay what what is the code in that function body um what functions call that function uh and you know what kind of types are in there what are those type uh definitions uh what what else calls that type elsewhere to try to reason about and figure out what the root cause of the of the crash is of the discovered vulnerability. So in the initially what we had in the semi-finals competition is we had something that was kind of a bit more fixed. So we tried to prescribe a context. We tried to reason okay well it's going to the LLM is going to need you know the stack trace information. It's going to need the function code you know caller collie to a certain degree a certain uh you know number of parents or grandparents or children or grandchildren of uh functions information about data structures and etc. It worked just fine uh for the semi-finals obviously but we started sorry just as a so this would just get shoved as part of the context and then you'd have the prompt of like use all this context but then make a patch for this thing. That's correct. And it's kind of a tall order to ask for an LLM, but we were we were very um measured in how we did this and through lots of testing as we as Michael had mentioned through the semi-finals. We were like, okay, well, we we are getting good enough results that we will perform what we expect to perform well in the semi-finals. Obviously, we did. So, we were pretty confident going from there. But, as Michael said, immediately after we we we found that we were in the top seven in the semi-finals, we went back and regrouped and said, "Okay, what if this scales? What if we get a bunch more money? what if we get a bunch more, you know, orders of magnitude, more repos, more complex code, you know, we have to prepare for the finals. So, um, we went back and figured out, you know, well, g giving an LLM everything is not, you know, or at least to to the degree that I was doing, maybe maybe not as the best, uh, decision. Maybe like the LLM guide itself. So, this is kind of a precursor. You can kind of make an illustration to like what MCPs are nowadays. Kind of a big thing. Waiting for for the more details to get there. So yeah. Yeah. Yeah. Of course. So MCP so we so to be clear um we are not using MCP in in in our CRS but um this is kind of a precursor to that. So basically you can think of this as a tool that the LLM can can attach to and and use. So in the in the prompt we'll say you know here's the stack trace and give it give it you know the information that we have from the POV and then we say hey you have these tools available to you. You can query for functions. You can query for certain lines of the code. You can query for specific data structures and what is involved in those data structures and where those are used and you get these tools to the LLM and the LM kind of goes through this discovery process and it's a bit more it's less of a prompt to give to the LLM. So the LM can be more targeted towards where it thinks it should go and discover as opposed to trying just to shove you know as we've said shove the entire Linux kernel all in there and say hey go figure out where the where the bugs are and patch them. Um, so again, it's a bit more of a judicious use of the LLMs and and as you know, Michael was saying, this obviously gets us to to the point where we're not using over overly using the LLMs, but using the LMS were appropriate. And um, again, in all of our testing, we were still seeing the same uh uh efficacy through the semi-finals challenges, but once we started running it on the the newer challenges, we were seeing a better a more effective solution uh through this. So um yeah, so back behind where the program model or the the contextualizer exists is just static analysis tools running. So the static analysis tools running are going to be the ones that actually are performing the the sort of program analysis. It'll figure out where the callers, colleagues, etc., etc. are and then that's just provided as a as a as a function call to uh the patcher and the C generator. And you're talking about stuff like clang indexer, code, these these guys. Yeah, something like that. Yes. So we we were using uh you know code query um tree sitter etc those types of things we tried to use so the the the dream was to use uh something graph-based and was working it was working well until Java came up and then Java didn't work really well for uh for some of the the things we were trying to I was trying to use you know y graph and you know all this sort of stuff and it just didn't the uh it just didn't work out for the program analysis tools that we were trying to use so we sort of ditched it and we went back to the uh the the just your general you know static based code analysis as opposed to it sounds like you chose the tooling specifically so that it would work on both C and Java. That's correct. Yes components. That's correct. Yeah. Because we wanted because as Michael said you know we we kind of underperformed uh you know on the Java side. So we wanted to make sure we're on a level playing field each way. So, you know, when we were doing our tests, we made sure that obviously we were still doing well in all the semi-final challenges and the the exemplar challenges we were being given for the finals, but we always had Java in our mind as equally weighted on on C challenge because we just didn't really know what the percentage of of those challenges were going to be. And honestly, like um yeah, we just we just wanted to make everything kind of under under the same umbrella, not get too specialized unless it was warranted. um you know we wanted to make design decisions decisions uh carefully um and with evidence. So I did a lot of testing right you know I mean before the the first exemplar uh to make sure that the design decisions that I was making actually did have an effect um and were still effective enough to to help the patcher and and the C generator out to uh to to patch and and and uh find find vulnerabilities or generate. Yes, this is super cool. So I think one of the things that uh sorry Yan that um kind of uh you know you said very specifically this is not MCP but is that just because you built this system and started it before that was clear that that was becoming the the kind of quote standard. That's correct. C MCP I I don't uh nec I'm not going to advertise myself as an expert on the genesis of MCP but I'll say it wasn't uh it wasn't readily available when we started it. So, uh, to be specific, I'm not using the protocol at all. That's what I mean when I say MCP. I'm using the concept of it, but I am not using it itself. That's a very clear like agentic style that you're allowing the LLM to use tools, which is essentially what MCP is supposed to, as I understand it, supposed to allow you to do. But yeah, I think what to me what's very interesting there is it just shows how quickly this field is evolving, right? like when you started this even I mean way before semis I don't think people were starting to talk about agentic like AIS like before semis but it was just like now it's a big thing but but you had to build this stuff over the last year you know two two years yeah it was super vindicating um doing this project for two years cuz like everything that we put into the semis like um became commonplace by the time we got to the um you know the end of the semi-finals and then some of the stuff we're doing now is like starting to really like um come about. So, for example, um uh our patcher has always been a strong suit. I'll get to the kind of the design of the patcher here later, but um a lot of what we put into the patcher um has later become what was baked under the hood for open AI and anthropics reasoning models. This like mixture of experts, this uh degree of prompting, separation of concerns, like we did all that before it was cool. And then um you know, our our first patcher was basically a multi- aent system. it now is like even more complex multi- aent system we did that before it was cool. So like you know all the stuff that you can do now in in Langchain easily like with stuff that we kind of like had to bastardize and and kind of wedge in. Um yeah and yeah like the way we structured our prompt like and the use of tools a lot of that stuff um you know that has become now commonplace for getting the best out of large language models like I feel like we did all of it um a year earlier than everybody else. So even just the concept, sorry Yan, just real quick, even just the concept of cons contextualizer, like there's kind of a a push I'm seeing among AI people that like we should not be thinking about prompt engineering, but context engineering. Like the important thing is what context you give the LLM, whether it's through tools or stuff that you pre-populate. And so that in itself, I think, is a very forward-looking thing. But go ahead, John. I I was wondering so so it's very forwardlooking but now as you said the field has kind of evolved now there is the the reasoning models and so on did you kind of go back and adapt your multi- aent or your multi-step separation of concerns chain of thought stuff to just use 03 and and and be kind of more straightforward or streamlined or whatever you might uh not as good maybe um or did you stick with with what you had actually no um we actually stuck with models that were were non-reasoning models. We eventually added them in I think as like backups or or um or fallback models and that was because through the semis and through the finals we built a lot of tuned to the problem of vulnerability discovery and patching. We built all of this problem breakdown all of this chain of reasoning chain of thought stepby-step multi- aent processes all the stuff that we built. So you know open AI and anthropic they've tried to generalize this and and and and bake this into the model by you know essentially like whatever query you provide to 03 you know that there's a there's a prior model that says here's what they've asked for how are you going to break it down how are you going to do it step by step and then figuring out which which fine-tuned model under the hood which expert model should handle this particular request or that particular request so you know thinking this from a computer science perspective best case scenario the 03 model is going to perfectly replicate what we've already done because we are experts trying to solve an unsolvable problem like no one's doing it at worst case scenario there are six people doing it better or six teams doing it better than us worst case we're the we're dead last and AICC but nobody else is you know really doing this so for we actually um still use GPT um I think 4.5 like we went with the later versions of the models but we went with the non-reasoning versions because they kept cost Americans are paying for. Yeah. Cuz four five is expensive but because you are careful about your usage you can actually afford it. I think we've talked to a lot of teams that say yeah for this we use uh uh GPT 3 or something 35 right because it's we need it to be dirt cheap because we query so much. Yeah. Um that's interesting because you're like this the 16 team we're talking to now we have a lot of this like um context other context. Exactly. So for example, theory um we were talking about a similar thing with with theory and they initially wrote their careful prompting strategy and if I remember correctly I think it was theory they threw it out uh faced with the same decision. Um, which is uh kind of an an interesting I I I liked your reasoning about about keeping also two teams that said that that they had some complicated patching where they broke like they had used two different models to like do the patch and then verify the patch and then when the thinking models came out they reran their initial test and was like wait this is this can just do it without us doing this other part. So, this is going to sound I was going to say this is going to sound catty, but um I say this mainly so I can I can I can pump up a member of my team. Ricardo Chiron was a member of our team who was responsible for the patch the entire way through and he's an incredibly diligent engineer. So, it was very easy for us to keep all of his patching components because they were well written and and pretty reliable. So in in other cases, you know, um especially when you have to put it together before the industry is ready to support it with, you know, standardized libraries and like it become like MCP and multi-agent are a thing. Um it's easy for that stuff to get complicated and become an albatross. In our case, it wasn't because we had an incredibly talented engineer who um who really owned that and and honestly, it's probably one of the strongest points of Buttercup is how well the patcher performs. I think if you actually look at the semi-finals, um, every vulnerability we found, we also patched. We didn't have anything that we left left points on the table after a V discovery. You know, we had issues in in and discovering vulnerabilities in in the Java side, but had we found them, I'm confident we would have patched them. And going forward, I can say this now, um, going forward, you know, part of this is just depending on when you find the vulnerabilities, and some of the vulnerabilities are more complex now. um going even now in in the exhibition rounds um it's pretty routine for us to patch like 80 to 90% of the vulnerabilities we find. So we're we are very strong on the patching side. So like I said, this is not to be and like you know you know tell everybody else they make they write spaghetti code and it's all crap. I'm really just trying to say this is my way of saying thanks to Ricardo for making me look good for like two straight years and well maybe we can then use that to transition into. So we have the contextualizer that the patching component can use. So then how does the patching component work? I mean we've basically been been addressing it but let's do it head-on. Yeah. So so our patching system is probably the most complex part of um the system. Um and this is largely because we didn't like you said least amount of compute, right? Because you said the fuzzing nodes are a ton of compute whereas patching was way less, right? Which I think is also super interesting to think about that like this part is very complex. Fuzzers are actually kind of simple, right? Once you know you're not writing the code for that using libraries. So anyways, yeah, please go ahead. Yeah. Yeah, sure. So the yeah, like I said, the patcher is super complex um and it's it's really cutting edge. So you know, right as um the semis were kicking off and we finally got an idea for what the what the what the CRS was actually going to have to do. This was when the first kind of uh papers were hitting not even not even like the actual literature like hitting archive out talking about you know like how to you know separate concerns and how to do these kind of like multi- aent systems and um you know so I' I've been doing um I've been doing like applied a IML for solving security problems for a long time back before LLM became the predominant form of technology and you know as we as we saw um LLMs come out you know you know particularly two years ago they were really trash at solving a lot of these problems um mainly because these problems are trained on these like huge generalized problems. So this was something that you know Ricardo and I when we discussed early on um you know we like we need to actually focus this problem down. The the prevailing mindset at the time was just throw all the code in there. This new new model X has a context window that's five times larger. So now you can fit all the code in, right? But that was presupposing that more context meant better results. And if you actually think about how large language models work, the more context you put in, the more you null down to the um now the more you null down to a like kind of like I call it like denominator. Yeah. Yeah. I call it the null weights. U the null waiting of your of your token probabilities. So you're actually better off and our experience has borne this out with um very highly contextualized, highly like narrow requests to large language models. So this was a design decision that was based in from the very beginning. So our very first patcher um had three distinct um agents. One was a leader agent which was very thin. It basically just kind of handled the the the work of the other two. Um it didn't it actually doesn't even use a large language model to do it. It's just code but it counts as an agent because it's in the agents folder of our source code. Um the other two agents are um a quality assurance agent and a software engineering agent. So the software engineering agent gets the the task of hey dude you broke this code or this code's broken it's got this vulnerability go fix it. So it tries to fix it. Then the quality assurance agent um was actually tool enabled. This is long before tools became you know part of like MCP and and and or just tool use in general um around it would actually we treated the competition infrastructure as and the actual repository as tools. So the quality assurance agent would be tasked with, hey, you need to go run the test and make sure that the tests come back clean. You need to compile this and build this. You need to make sure that it builds. So it actually had like kind of built into the prompting and built into some of the automation this validation step that made sure that when we submitted things, we never submitted anything that was wrong because I mean it is possible to validate every POV and every um patch that you submit. So, in our finals version of the CRS, we added a couple of other agents that do some small things, but for the most part, it's pretty much the same. Like, we have a persona that is a software engineer that is tasked with fixing the code, not just you're a general LLM or you're a security expert. It turns out if you ask software like the soft the the terminology for software engineering is much more associated with with coming up with corrections than um than than vulnerability researcher which actually you know the more I think about like most of the vulnerability researchers and software engineers I know I actually probably would trust the software engineer to fix the code more we're good at we're good at finding the the bugs and pointing out everyone else's mistakes but thank god we don't actually have to write our own code like at scale or or that level. So we we'd probably be be shown to be uh just as bad as like your anecdotal experience. Did you run like daton uh experiments? This is super fascinating to me because you know of course people blob you're a cyber security expert and they don't really think about it. It's anecdotal. We we've done some work for the UK government on like evaluating models for their ability to do certain tasks. And I think you know early on we said like you're a cyber security expert and then we we asked it to do things like find vulnerabilities and we didn't get great results but then you know we found um through just like kind of prompt engineering that that separating a quality assurance engineer that was responsible for like kind of looking at the quality of the code and then a um a software engineer that actually was responsible for generating the patches like calling it a software engineer was was was better. Um, so yeah, like I said, we had a couple of other um kind of agents that did other uh smaller tasks. Um, we had one one agent that was responsible for like reflecting on a patch. Um, some of these smaller agents were um kind of created to kind of fine-tune towards the end of our um of our final submission. For the large part, you know, we really only needed a three agent system with access to lots of tools. Um, and when I say lots of tools, I don't mean like custom stuff. I mean, just like the ability for it to um on its own run the test because otherwise you have to like rely on these really complicated automation loops where you're relying on this thing to like recognize that like it gives you something. You have to test it. Okay, it's trash. Now you have to figure out what to what to provide it back. A lot of this infrastructure exists now and is actually easier to run and maintain like a a coherent context window when it's run as a tool versus um being run completely separately and you're using like manual Python code to to pipe information around. Awesome. Okay. So then that and the that verified patch and probably POV and all that information goes back into the orchestrator. So then how does it know what things to submit and when? Yeah. So this is where the game is going to ship kind of. So yeah. So so the patcher uh the patcher when it finds good patches it sends them up. Um and uh the orchestrator keeps track of the various patches that we have. We don't really wait on submitting patches. we submit them as as fast as we can because of the time component and scoring. Um but then the presence of patches then kind of informs um future iterations of the of the CRS. So for example, if we have two POVs um that don't show up as duplicates b or don't register as duplicates based on our um based on our dduplication scripts that we largely source from the competition organizers, we actually test that POV to see if it's fixed by patches we already made. And if it if it is fixed, then we register as a duplicate that way. So we avoid processing it and potentially submitting a duplicate patch, which does actually cost you points in the form of accuracy. You know, the competition organizers, they kind of like nerfed the accuracy multiplier um with the last uh round, but um but still, you know, we we recognize that it could be a significant um a significant scoring detriment. So our our scoring strategy was basically don't lose the points that we get and do a good job of getting points and then we probably do okay. Um yeah, it's pretty interesting like when you think about it there's because you talked about you know I think the stoastic nature of fuzzing, right? So there's the kind of the key components I think in the scoring where this affects things is the speed at which you find things. So you're rewarded the faster you find things which went into your patching strategy like submit the patch as early as possible to get that speed bonus. And then there's the accuracy multiplier. So assuming you know I think actually talking to the teams there's enough diversity in the systems themselves I I think there will be pretty disjoint sets of of POVs and patches found but among similar systems that are finding the same bugs those differences of speed and accuracy multipliers will kind of probably be the make or like the differentiators right yeah that's part of the reason why we um why we why we do like POV and patch cross comparison because We also test to see if a POV that we found that is registered as a duplicate. We test to see if it breaks a patch we already made. And if it does, we know we're not going to score anything with that patch. So at that point, it's now more advantageous for us to submit a new patch, which we get dinged on the accuracy multiplier, but we actually collect points and the patch is worth more than anything else in the in the system. So like if we get to a situation where we know a priori um that we found a vulnerability, we we can be sneaky and not submit it and then not not bolo our own our own patch. But we we figured with six other teams out there, someone's going to find this POV because we found it with fuzzing. Like it's not like we did anything super special. Everyone's using, you know, OSS fuzz. Um so someone else is going to find it. So that then we have to go back to the drawing board, repatch, but now we're patching against multiple POVs instead of a single one. Um, and does the patcher use those additional POVs? Like if it's generating a patch and it knows the POVs are related, it will then use that. Does use that as part of context or just checking that the generated patch actually fixes all of those POVs? It's both. It's both. So the um the patcher gets access to the different POVs um and then also it goes into the quality assurance routine for um you know making sure that all the vulnerabilities are patched before we um before we submit um before we go again. So, we we never actually end up submitting that other like offending POV. Uh, actually, no, we do submit the offending POV. If we find one that clobbers our own patch, we submit it because we hope we're going to clobber somebody else's, too. So, we're kind of we're kind of mean in that way. We chose to we chose to to be uh to take everybody else down with us. If we're going to lose our accuracy multiplier, then we're going to be jerks about it. No, it makes sense. Yeah, that's I think we did we go the other way, Yan, and and hope that somebody else would submit that. I think there was one case where we if we knew we had a POV like a bypass for a patch that we were never able to fix that POV. I can't remember. This sounds I Our strategy was very complex. I can look it up in our docs, but it's probably not too complicated. Who knows what the system did. Hopefully, it did what it was supposed to do. It is super complicated. Like I described this like this functionality like it was always part of it. like this was stuff that made it in like a week before the competition was going. So we we we were scrambling and like honestly like most of the work we did in uh in June after the third exhibition was focused on just adjusting to the changes in scoring uh in scoring that were published by DARPA. So um that was the vast majority of uh of what we changed in June. By by the time we got to the end of the third exhibition round, we we were basically like pencils down on the actual core functions of the CRS, which was kind of interesting. Um uh it was it was a very different flavor work that last two or three weeks that we had before the before the final. Very cool. All right. Awesome. So, yeah. What would you consider to be your buttercup secret sauce? Yeah. So the secret sauce I would say I've said this quite a few times in in various places but we we started this off with we always had a best of both worlds approach. You know I personally had worked in applied AI for a while before this had no delusions. I've seen firsthand how hard it is to make uh like a machine learning model like even remotely be good at vulnerability detection. I did it on a prior DARPA program and um Evan and I like this one of the papers that we published together while we were both at Georgia Tech. So we knew going into this that like AI, machine learning, whatever you want to call it, it was going to be really bad at certain things. Then we also have been, you know, my my background is actually in compilers and Evans is in um malware analysis. Like we we both have do a ton of work in conventional software analysis. So we know exactly where the halting problem is and and how how it wins every day. It wins every time. So we knew that program analysis was going to suck at certain things. So particularly on real world stuff, right? I think that's the other, you know, not on like toy small problems. This on real world software that is crazy. Yeah, patching is the patching is that spot like there's all kinds of program synthesis approaches and all this kind of, you know, stuff that people made to to try and tackle like software patches. They are all terrible. um when it comes to fully scaled programs. I'm not crapping on anyone's work. You got to solve you got to solve you know um you know Linux core utils before you solve you know Apache Tikica or before you solve the Linux kernel. But um but yeah there's a scalability problem there and that's one of the areas where AI was was a good place. Yeah, really when I when I sat down to write our original concept white paper, um like I said, I did this with um with uh the other person who wrote this with me, Ian Smith, um we sat down and we kind of we said, "What what tasks are you going to need to perform and what is what is good at each of these and what is good at each of these for certain types of problems?" So, if you look at our original concept white paper, you know, it really wasn't a pipeline. It was like more like kind of like a giant matrix where something would go in, we'd characterize it, we'd choose the right tool, we'd push it to the next stage, choose the right tool based on what tool we used previously. It ended up getting consolidated down to a single pipeline. Um because we we got a lot of constraints that were solved um by the competition infrastructure. Um you when we set this down, we said, okay, what is good what the software analysis already do? Well, fuzzing is a good example. Like if you need to find a POV, people have been researching fuzzers since the 80s. I think it was when the first paper came out. There's a little bit of a gap between before it got serious, but like it's almost like a right of passage getting a PhD. You have to write one fuzzer. You do not get a PhD in computer science or security without writing at least one fuzzer at some point somewhere. So people love to write fuzzers. There's a million of them and they're really really good at finding vulnerabilities. So, we didn't look at like how are we going to make the LLM do this better or how is the LLM going to or how are how is AI even more generally before we knew that we were going to be kind of mostly limited to using large language models. Um, at least for the semis, you know, we we we were looking at how we're going to try and like make AI do something it's bad at. Um, and similarly, we weren't we didn't have any basically we went into this with no delusions about the capabilities of either set of tools, whether they are AI or whether they are um conventional. and we chose to mix and match them basically where they where they go. Um I don't know I I use this I use this analogy that I'll share here that I I usually use is to try to like explain to um you know junior researchers like you know when to use each um you know when we talk about conventional software analysis it is a prescriptive solution you are prescribing a series of steps to the computer to go solve your problem. It works well when you know what to do and things flow from one step to the other. It's like any program. A IML is a descriptive solution. You're giving it lots of examples and you're saying I'm describing how the solution looks for a particular problem and I'm asking the model to infer what the solution will look like based on the descriptions I've given it. Certain problems are prescriptive. Certain sol certain problems are descriptive. And your ability to solve them should govern when you're going to use AI, when you're going to use conventional stuff. And we use that. We stuck to our guns on that. the AI hype machine went insane. Um, our company's embraced AI. We use it probably more than anybody else, but um, we have always stuck to our guns on like we're not going to use it in a place where we're going to go off and go on some wild goose chase. We're going to waste a bunch of time. Just like when we when we try to solve computer science problems, every computer scientist, you know, that that has a graduate degree knows how to reduce a problem to figure out if it's traveling salesman or the halting problem. Like traveling salesman means like MP complete. Okay, I can probably solve this, but it's going to suck doing it the whole time. And if it's halting, like no, stay away. You don't you're you're going to waste a year of your life doing this. A IML is kind of the same way. So like it really comes down to problem formulation. We've stuck to that. That's the secret sauce of buttercup is we did a lot of just exhaustive problem formulation on what is the right tool to use for this particular job when the constraints are fully automated you can't touch it. So this is great. So then how so during the competition right then how did your outlook change on AI like by participating in and as the models change. So looking back, did it kind of like the problems that you looked at where you're like, "Wow, the AI, you used to be bad at this, but now actually is good at it or better than I thought." Um, so the answer to this is that I am currently sitting in a bubble full of a tremendous amount of confirmation bias and that I went into this knowing what problems the AI was going to be good at. Um and I only used them or we only used them there and um in both cases both for patching and for CG uh seed input um creation. Um my expectations were exceeded in both like both of them really surprised me like okay these are actually doing a lot better. I was really kind of pessimistic about it going in but they both performed you know much better than I than I thought they would. Um, but the reason why I say I'm in this bubble of confirmation bias is it's entirely possible we go to this competition, we get absolutely smoked by somebody who used AI for like a purpose that we thought was like poorly fitted for it and it turns out it wasn't, you know, so like the element of surprise is is potentially there to bite us in the ass. So, um, so yeah, for now it looks great. In hindsight, I wouldn't have changed anything because the stuff I decided to use it for worked out great. Uh, but I also don't know if I missed anything out there where it's like a really killer application of AI that I I missed due to lack of imagination. But at the very least, we didn't go down any rabbit holes. We didn't waste a lot of time trying to make AI do something um that it really wasn't suited for. Um, Evan, what do you what do you think? Do you have a different answer to this or what are your thoughts? I've been talking for a long time. No, I'll pair what you said. But I think that yeah, I think again the the I think one of the best compliments we received in the ASC at the end of the the semi-finals was that we had the most jiu-jitsu use of LLMs. And I think I'm pretty proud of that. So no, I will I will I'll just I'll par whatever you know what Michael just said and I'll I will say I I have we have seen a um a noticeable improvement in in the different models abilities to patch. Uh there were there were three different models we were you know using in sort sort of a roundroin uh uh type manner or or some sort of uh in the semi-finals in case one didn't do as well we switched to a different model and it's you know there's kind of like a convergence you know earlier this year there's a lot of uh new major models I mean obviously you know open AI you know GPT 41 came out and um you know anthropic had uh had um had had cloud 4 come out and so there's there's been some very noticeable changes and I I think um again because of our focused use of LLMs um I think we we did notice a difference at least from our own testing and infrastructure. Were you like very pro AAI at the beginning and now you're even more so or were you did you were you skeptical at the start starting this new job and being like wow I have to make these AIs work? No. No. I mean so I I'll again I have the same attitude Michael has. Uh so in my my research back when I was in grad school was um a ml applied to cyber security so specifically for malware analysis and malware detection. So I mean semester one I mean paper one the halting problem is you know ground into your head and and and understanding what these things can and cannot do. Um no I think I have I think I I had and I still have the same attitude towards it. Um I think it's a useful tool. I think it has a place. Um but I think again it's it's a application needs to be um measured. I think it needs to be I think we need to be diligent with how we're using it and not just you know yoloing. Uh just give every get everything and hope everything sort of works out. Um I I don't like rolling the dice that much. Um but obviously there is a place for it. So no I don't think my attitude has really changed um on AI since this competition. But it is interesting. It's been it's been fun to see how how well it has it has done and how it's performed to patch uh patch various software bugs we found. Yeah, that's great. Yeah, I think that'll be that's one of the interesting things that are coming out of these interviews that we're doing. Uh for instance, uh it was theory, right, Yan that is actually submitting patches to which they don't have a POV for. So they found their confidence in the in in the LLM's assessment of buggginess. Yeah. So that'll be I mean just like uh you said Michael I think it'll be super interesting seeing the results how that shakes out how the different approaches to the problem will will manifest. Yeah it's I I I really appreciate that confidence of of the the depth of understanding of the approach. Um that's a very unique secret sauce. Um, and it's also cool that, you know, looking back, it sounds like you have no major regrets, major changes you'd you'd make. Um, I mean, I might have some regrets on August 9th. Well, the good news is I'm going to be That's why I'm asking you now before before you know the results. Exactly. Yeah. Yeah. You know, twice the pride, twice the fall, right? The good news is, you know, we're going to be in Vegas to hear the uh the results come out. So if we are indeed disappointed, we are in the perfect city to handle sorrow and and and deep existential regret. But no, yeah, I don't think we do anything differently like um you know like admittedly like our our approach is kind of conservative. We focus on doing a really good job at scoring points and doing a really good job at not losing them. So when it comes to seraps, this is a good example of this. We actually didn't do a whole lot with serif broadcasts because they're not worth very many points on their own and you only get points for bundling them if you also found a POV and a andor a patch and you really only get like the huge reward if you found both. So we actually wait to handle Serif broadcasts until we've already patched a vulnerability and it's only if we see overlap between the reported code region and the serif with either the localized vulnerability or the um region that we patched in code. That's the only time we actually handle those or bundle those. So, it's kind of stark contrast to yeah, we don't submit any patches without a POV um because the you know the potential for your your accuracy multiplier to get damaged there. And then also, you know, if you bundle those um if you bundle that well, I guess if you don't have a POV, they they probably wouldn't bundle it. But uh but yeah, the accuracy multiplier, I guess that being nerfed, it kind of makes strategies like that possible um to to go out there on a lark and and hope that you're um that the LLM is is properly assessed everything or properly patched a vulnerability that you can't trigger. But yeah, we took a pretty conservative approach and that's mainly because um I really wouldn't call it conservative. We're really aggressive with with how we go out and find vulnerabilities. Like we're trying to use our resources to the absolute maximum. We did a lot of work actually just doing engineering tuning on like how many replicas of each type can we get to spend almost the full $80,000 in the final and spend all the LMS. Um yeah I think our other secret sauce is probably like engineering. We had really good talented engineers. So like I think we probably worried the least among everybody about like uptime, reliability, that kind of stuff. We tested for it pretty extensively. From what I'm hearing, it kind of makes sense your whole, you know, I think maybe if I had to term a philosophy or something, it's around engineering focused, right? So, you started out at the beginning planning the system based on which components and which techniques you thought were best. You engineered the system, had it so that you had CI running like it was a well- tested system that did like all the components should have been doing what they were doing. um and conserv you know I think conservative I don't you know I guess with the use of LLMs and stuff like I see that as like we're not going to invest time in crazy researchy stuff that may or may not pan out and may balloon our LLM usage or something but we don't quite know if it's actually going to help us win like let's focus on on solidifying the core is kind of the what I took away from it. Yeah, I think that's a really fair description. So then the $4 million question, where's all of this going to go? What place do you think you're getting? You know, I think we're in the, you know, 51st percentiles, you know, somewhere between, you know, one and four. I think we'd probably be pretty surprised if we ended up in the bottom half. A lot of this is just based on prior performance. We did really well in the semis. Um, we found a lot of bugs first. We patched a lot of bugs first. We had some obvious issues with um with finding Java. the Linux kernel was a disaster. I'm not even counting that as like a CP. I kind of black out when I think that it was even involved in the first place, but um fortunately it didn't show back up. Um but yeah, when it came to the Java examples, like we had some like we had engineering issues that prevented us. So maybe that was why we were hyperfocused on engineering in the second half to to make sure we didn't leave stuff on the table. But um yeah. Were there any uh I think you mentioned it before with the file names. Were there any other kind of like close call bugs that made it into the system that you you're testing caught and that you're now very happy that you found it and Yeah, that was um that was the worst one that made it in that actually went to a round that we didn't find in advance. The finite one. Yeah. Yeah. And that one was had a lot to do with um just like needing to see new examples. needed a we needed a stack trace that was long enough that when you when you when you have or when you you know compute it into a file name that it exceeds the length. So yeah, we had a couple of other like close calls here or there with um stuff that we had to patch like a day before we went in, but we were really good about um being pencils down several days at least on the branch that we were going to submit um pencils down a couple days before. So our our typical approach to a deadline for an exhibition round was, you know, there were usually the deadlines were usually like on Wednesday or Thursday. We would be pencil down on Friday. We'd run um a full-scale test over the weekend. We'd come back. Whatever issues that broke, we would fix on Monday, run another like shorter full scale test on Tuesday, Wednesday, fixing only those like small bugs that we had high confidence in and then push out. But that being said, we have a we have a couple of bugs that like or a couple of Heisen bugs that they they are absolutely in there. We we over log everything. If you look at our logs, we look like a bunch of sloppy jerks who who don't care about software engineering, but you know, those a lot of that stuff was just like we don't have time to fix everything. We're on a we're on a tight deadline, so not everything got fixed. But they're they're all like minor stuff that like um either doesn't affect the operation of the CRS or absolutely at the absolute like worst case like after running for you know seven days it might make a node fail but the but the case will just restart it and you're right back to being good. I think the other superpower we kind of had to lead up to that because we only, you know, experienced one major issue during one of the exhibitions is because um each person kind of owned a component of the system and we'd have regular meetings to discuss it. And I think one of the best parts of this is we were always still like connected to each other and reviewing each other's code. One of the policies we had on the repository is that you couldn't merge your PR until someone else reviewed it. at least even if it's not not their component. And I think what that led to was a lot of accountability and a lot of gut checking and cross-checking um from other people that not they're not even in charge of your component. So even though I didn't do any work on uh let's say or a lot of you know heavy lifting on like let's say the fuzzing or the patching you know I was asked to do different poll requests. So I it gave me a lot of opportunity to ask lots of questions and kind of kind of gut check the design decisions and these things. So, I think a lot of, you know, having your own ownership and then also getting um getting double checked by your teammates was also like what led to us not experiencing that many issues leading up to the exhibition challenges. Yeah, that's great. It was probably helpful even having eyes on and and knowledge of other components. So, if they did hit issues or you know, you could you have some redundancy built into the software engineering process in some sense. Very cool. Yeah. Any other Yeah, please go ahead. Yeah, the the the question the audience clamors for. Who's your favorite AI? Ah, we just talked about anthropomorphizing AIs on this episode. I know. That's why I said, "What is your favorite AI?" Okay, there you go. Man, I mean, I don't know. My favorite AI is whichever one happens to do the best job of of solving the particular task because like they all seem to be different. Um, I don't know. I'd say generally right now I probably use um Claude for more uh tasks than than anything else. Um really good at code generation. Um so I don't know like the great news is we have like a super diplomatic answer if the if you're asking like you know how does our CRS use it? Like we were split 50/50 between open AAI and Anthropic. Um, anthropic is we use that primarily for CGE and it's the fall back for patching and OpenAI is the primary for patching and the and the fall back for um fall back for CGE. So I I think we we we have officially like managed to by by sheer luck avoid offending either of the major like tech players. You know, Google might be mad at us because we didn't use Gemini, but we did use Gemini 1.5 as a fallback in the semis. Um so they um um so it was helpful there. Um, but yeah, honestly, I'd say I probably use Claude more than anything else. But, um, I I admit like I I'm I'm probably pretty sparing at AIUS to be completely honest. And that's just mainly due to like the nature of the work we're in. We're we're in the business of novelty. Like we find novel bugs, we find novel patches. Like everything we do is new and hasn't been seen before. And that is not something you know LLMs are good at doing when it comes to the problem domain of cyber security because vulnerabilities, software bugs, they don't exist on like this differentiable curve that you can match like they they're all like these just like fundamentally different concepts. So um so yeah, for my work I I don't use it a ton, but I I use I use I use cloud probably the most. Evan, what do you got? Uh I'd probably parrot what you said. I think because I think we're mostly, you know, use uh there's there's some codeisters that we use as a part of our company and I think most of most of the time I'm probably using uh Claude and then if I just need to riff some thoughts out, I'll probably use um I' I've been using Gemini. If I need some like sort of free form, like I have this concept of an idea, like I got to sort of flush this out. I need some non-determinism in my thought process. I'll just throw it to Gemini and see what it what what happens. Um I will say it was fun. I actually had to do like a I had to I had to I had to get I have to give a talk in a couple months and uh I just like recorded myself talking in like just an audio clip of what I wanted to say in my presentation and uploaded that plus like the report that I co-authored uh to I think it was Quad Sonnet and it spit out this like I was like hey I need 10 slides I need I need them to cover these specific key points and it gave me this kind of cool like just boilerplate outline of like hey on this slide talk about these three points because you mentioned this and So, you know, it's kind of a nice thing to kind of I guess you you can throw it to to get started. I guess it's a it's a good starter for that kind of stuff. It's the it's the professor trick. It's so hard to write a paper when it's blank. It is so much easier to shape and edit a student's paper that they have written. It's it's shocking like how much that difference is like and there's also experience and all that, but uh I mean Yan knows cuz he just ve very recently wrote co wrote a first author paper and I don't know what I was thinking. Yeah. Yeah, for the first time done with that over a decade. No, eight years. Wow. Well, see you'll get there. I thought that was the whole point of I thought that was the whole point of of getting a tenure track position is you never have to do that again. That's all your students. I was thinking flip side of a tenure track position is you have the freedom to make stupid mistakes like writing a first paper. It's so much work. We ask our students to do this all the time. It's so much work. You forget about it. you you know only the good memories like yeah we finished it was published the the actual track at 5 a.m. like 4:47 a.m. 13 minutes before the deadline. Like, why did I do this to myself? On the flip side, that's their full-time job. You also have are doing several things. So, that's right. A little bit of a difference there. I had a meeting today. I'm dead. Yeah. So, I realize it's backtracking a bit, but I do want to give like one AI pro tip for for for the fellow competitors who who probably watch this. Um, when you go to open source your um, CRS, um, Claude and and Cursor are actually exceptionally good at helping you find all the comments that you don't want people to see and help you delete them. Took about 30 minutes to get through both of our versions of it, but there was definitely some stuff in there that was like, you know, why the does this work or who wrote this or stuff like that. So, what you want in there? I was just going to say I was pushing the team. The team wants to like do a nice sanitized open source release and redact the history and I'm like show the people the guts of the sausage. I they want to see all the crazy comments where you're being like why the f doesn't this work and like magic hack to get this thing to work. And uh yeah, I I was also pushing for that for CTF challenges, but nobody likes to show the history. They just want to see the the finished product. AICC is too high profile. I don't want to get all the nerd sniping after after this. And that's 100% correct. I think that's totally fair. And I wrote very little to none of the code, so it doesn't affect me at all. Nice. One of the other benefits of being a tenure track professor, right? Yes. Yes. We were Yan and I were in advisory roles, uh, you know, helping with helping out when when stuff needed needed helping out. But cool. Everyone went hands-on, but you know. Yeah. Exactly. You got to Yeah. All right. Well, thank you so much for Trailer Bits and specifically Michael and Evan for joining us today. I'm Adam D. You can find me online at Adam Dupe. He's Zardis. You can find him online at zardis. Together, we're CTF radio and you can find us online at CTFradio. Uh you can send us questions through email at ctfradiogmail.com. And your questions might end up on a future episode of CTF radio. Take care and happy hacking. There we go. All right. Don't leave. It'll upload. I think everyone was great. No, we're not. Oh, I hit this. Oh, it's always bad when I hit a button and then I got No, we should keep this in the in the episode. 